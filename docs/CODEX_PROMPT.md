# Codex å¼€å‘æŒ‡ä»¤ï¼šMarkdown SEO è¯Šæ–­ Agent

## é¡¹ç›®æ¦‚è§ˆ

ä½ éœ€è¦å®ç°ä¸€ä¸ªPythonå‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨äºè¯Šæ–­Markdownæ–‡ä»¶çš„SEOè´¨é‡ã€‚è¿™ä¸ªå·¥å…·ä¸æ˜¯ç®€å•çš„è§„åˆ™æ£€æŸ¥å™¨ï¼Œè€Œæ˜¯ç»“åˆäº†é™æ€è§„åˆ™å¼•æ“å’ŒAIè¯­ä¹‰åˆ†æçš„åŒå¼•æ“ç³»ç»Ÿã€‚

**æ ¸å¿ƒç›®æ ‡**ï¼šç»™å®šä¸€ä¸ªMarkdownæ–‡ä»¶ï¼Œè¾“å‡º100åˆ†åˆ¶çš„è¯Šæ–­æŠ¥å‘Šï¼ŒæŠ¥å‘ŠåŒ…å«å…·ä½“é—®é¢˜ã€æ”¹è¿›å»ºè®®ã€ä»¥åŠAIå¯¹å†…å®¹æ·±åº¦å’Œå¯è¯»æ€§çš„è¯„ä¼°ã€‚

**å…³é”®çº¦æŸ**ï¼š
- LLMè°ƒç”¨å¤±è´¥æ—¶ï¼Œç³»ç»Ÿå¿…é¡»é™çº§ä¸ºçº¯è§„åˆ™åˆ†æï¼Œä¸èƒ½ç›´æ¥æŠ¥é”™
- æ‰€æœ‰SEOè§„åˆ™éƒ½é€šè¿‡é…ç½®æ–‡ä»¶ç®¡ç†ï¼Œä¸å…è®¸ç¡¬ç¼–ç é˜ˆå€¼
- ç”¨æˆ·æœªæä¾›å…³é”®è¯æ—¶ï¼Œè‡ªåŠ¨æå–Topå…³é”®è¯ï¼ˆåŸºäºn-gramåˆ†æï¼‰
- è¯Šæ–­æŠ¥å‘Šå¿…é¡»æ˜¯äººç±»å¯è¯»çš„Markdownæ ¼å¼ï¼Œä¸æ˜¯JSON

## ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡

**åŒå¼•æ“æ¶æ„çš„åŸå› **ï¼š
è§„åˆ™å¼•æ“å¤„ç†å¯é‡åŒ–çš„æŒ‡æ ‡ï¼ˆæ ‡é¢˜é•¿åº¦ã€H1æ•°é‡ï¼‰ï¼ŒAIå¼•æ“å¤„ç†ä¸»è§‚è´¨é‡ï¼ˆå†…å®¹æ˜¯å¦æ»¡è¶³æœç´¢æ„å›¾ã€å¯è¯»æ€§ï¼‰ã€‚ä¸¤è€…äº’è¡¥ï¼Œè§„åˆ™å¼•æ“ä¿è¯åŸºå‡†è´¨é‡ï¼ŒAIå¼•æ“æä¾›æ·±åº¦æ´å¯Ÿã€‚

**é…ç½®åŒ–è§„åˆ™çš„åŸå› **ï¼š
ä¸åŒåœºæ™¯çš„SEOè¦æ±‚ä¸åŒã€‚æŠ€æœ¯åšå®¢å’Œè¥é”€é¡µé¢å¯¹å…³é”®è¯å¯†åº¦çš„è¦æ±‚å®Œå…¨ä¸ä¸€æ ·ã€‚ç¡¬ç¼–ç ä¼šå¯¼è‡´ç³»ç»ŸåƒµåŒ–ã€‚å‚è€ƒ`/mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/seo_rules_config.py`çš„è®¾è®¡ï¼Œæ‰€æœ‰é˜ˆå€¼éƒ½å¯ä»¥è°ƒæ•´ã€‚

**è‡ªåŠ¨å…³é”®è¯æå–çš„åŸå› **ï¼š
å¤§å¤šæ•°ç”¨æˆ·ä¸çŸ¥é“åº”è¯¥ç”¨ä»€ä¹ˆå…³é”®è¯ã€‚ä¸å…¶è®©ä»–ä»¬çŒœï¼Œä¸å¦‚ç³»ç»Ÿæ™ºèƒ½æå–ã€‚å‚è€ƒ`/mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/analyzer.py:185-235`çš„å®ç°ï¼ŒåŸºäºè¯é¢‘+è´¨é‡è¿‡æ»¤ã€‚

## ä¸Šä¸‹æ–‡æ–‡æ¡£

åœ¨å¼€å§‹ç¼–ç å‰ï¼Œä½ å¿…é¡»ç†è§£ä»¥ä¸‹æ–‡æ¡£ï¼š

1. **PRDæ–‡æ¡£**ï¼š`/mnt/d/VibeCoding_pgm/MD_Audit/docs/PRD.md`
   - åŒ…å«å®Œæ•´çš„è¯„åˆ†é€»è¾‘è¡¨æ ¼
   - ç”¨æˆ·åœºæ™¯å’Œå·¥ä½œæµç¨‹
   - LLM APIé…ç½®ç»†èŠ‚

2. **æŠ€æœ¯è®¾è®¡æ–‡æ¡£**ï¼š`/mnt/d/VibeCoding_pgm/MD_Audit/docs/TECH_DESIGN.md`
   - å®Œæ•´çš„ç³»ç»Ÿæ¶æ„
   - æ‰€æœ‰æ¨¡å—çš„ä»£ç æ¨¡æ¿
   - æ•°æ®æ¨¡å‹å®šä¹‰

3. **å‚è€ƒä»£ç **ï¼ˆä¸è¦ç…§æ¬ï¼Œç†è§£æ¨¡å¼ï¼‰ï¼š
   - `/mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/analyzer.py`
     - ç¬¬16-94è¡Œï¼šå…³é”®è¯è´¨é‡è¿‡æ»¤é€»è¾‘ï¼ˆæ‹’ç»URLç‰‡æ®µã€ä»£ç ã€æŠ€æœ¯æœ¯è¯­ï¼‰
     - ç¬¬185-235è¡Œï¼šn-gramå…³é”®è¯æå–ï¼ˆunigramsã€bigramsã€trigramsï¼‰
     - ç¬¬653-696è¡Œï¼šæ ‡é¢˜å’Œæè¿°éªŒè¯é€»è¾‘
   - `/mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/seo_rules_config.py`
     - å®Œæ•´çš„é…ç½®ç³»ç»Ÿè®¾è®¡ï¼Œdataclass + JSONåŠ è½½
   - `/mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/page.py`
     - Pydanticæ•°æ®æ¨¡å‹å°è£…æ¨¡å¼

## å®ç°é˜¶æ®µï¼ˆ7ä¸ªé˜¶æ®µï¼‰

### Phase 0: é¡¹ç›®è„šæ‰‹æ¶ï¼ˆ15åˆ†é’Ÿï¼‰

**ç›®æ ‡**ï¼šåˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„å’ŒåŸºç¡€é…ç½®æ–‡ä»¶ã€‚

**æ‰§è¡Œæ­¥éª¤**ï¼š

1. åˆ›å»ºç›®å½•ç»“æ„ï¼š
```bash
cd /mnt/d/VibeCoding_pgm/MD_Audit
mkdir -p md_audit/{parsers,engines,models,utils}
touch md_audit/__init__.py
touch md_audit/{config.py,analyzer.py,reporter.py,main.py}
touch md_audit/parsers/{__init__.py,markdown_parser.py}
touch md_audit/engines/{__init__.py,rules_engine.py,ai_engine.py}
touch md_audit/models/{__init__.py,data_models.py}
mkdir -p tests/{fixtures,unit}
touch tests/__init__.py
```

2. åˆ›å»ºä¾èµ–æ–‡ä»¶ `requirements.txt`ï¼š
```txt
pydantic>=2.0.0
python-frontmatter>=1.0.0
markdown>=3.4.0
beautifulsoup4>=4.12.0
openai>=1.0.0
pyyaml>=6.0
```

3. åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶ `config/default_config.json`ï¼š
```bash
mkdir -p config
```

å†…å®¹å‚è€ƒæŠ€æœ¯è®¾è®¡æ–‡æ¡£ä¸­çš„é…ç½®JSONç¤ºä¾‹ï¼ŒåŒ…å«ï¼š
- `title_rules`: min_length=30, max_length=60
- `description_rules`: min_length=120, max_length=160
- `keyword_rules`: min_density=0.01, max_density=0.03, max_auto_keywords=5
- `content_rules`: min_length=500, min_h1_count=1, max_h1_count=1
- `llm_api_key`: ç•™ç©ºï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼‰
- `llm_base_url`: "https://newapi.deepwisdom.ai/v1"
- `llm_model`: "gpt-4o"

**éªŒè¯æ£€æŸ¥ç‚¹**ï¼š
- [ ] è¿è¡Œ `tree md_audit` ç¡®è®¤ç›®å½•ç»“æ„æ­£ç¡®
- [ ] è¿è¡Œ `pip install -r requirements.txt` æˆåŠŸå®‰è£…ä¾èµ–
- [ ] `config/default_config.json` æ ¼å¼æ­£ç¡®ï¼ˆç”¨ `python -m json.tool` éªŒè¯ï¼‰

### Phase 1: æ•°æ®æ¨¡å‹ï¼ˆ30åˆ†é’Ÿï¼‰

**ç›®æ ‡**ï¼šå®šä¹‰ç±»å‹å®‰å…¨çš„æ•°æ®ç»“æ„ï¼Œç¡®ä¿æ•´ä¸ªç³»ç»Ÿçš„æ•°æ®æµæ˜¯å¯é¢„æµ‹çš„ã€‚

**å®ç°æ–‡ä»¶**ï¼š`md_audit/models/data_models.py`

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨Pydantic v2çš„ `BaseModel` å’Œ `Field` è¿›è¡ŒéªŒè¯
- æ‰€æœ‰å¯é€‰å­—æ®µå¿…é¡»æœ‰æ˜ç¡®çš„é»˜è®¤å€¼ï¼ˆé¿å… `None` æ­§ä¹‰ï¼‰
- æšä¸¾ç±»å‹ç”¨äºå›ºå®šé€‰é¡¹ï¼ˆå¦‚ `severity`ï¼‰

**æ ¸å¿ƒä»£ç æ¨¡æ¿**ï¼š

```python
from pydantic import BaseModel, Field
from enum import Enum
from typing import List, Optional, Dict

class SeverityLevel(str, Enum):
    """è¯Šæ–­é—®é¢˜ä¸¥é‡ç¨‹åº¦"""
    CRITICAL = "critical"  # ä¸¥é‡å½±å“SEO
    WARNING = "warning"    # éœ€è¦æ”¹è¿›
    INFO = "info"         # å»ºè®®ä¼˜åŒ–
    SUCCESS = "success"   # ç¬¦åˆæœ€ä½³å®è·µ

class DiagnosticItem(BaseModel):
    """å•ä¸ªè¯Šæ–­é¡¹"""
    category: str = Field(..., description="ç±»åˆ«ï¼šmetadata/structure/keywords/ai_semantics")
    check_name: str = Field(..., description="æ£€æŸ¥é¡¹åç§°ï¼Œå¦‚'title_length'")
    severity: SeverityLevel
    score: float = Field(..., ge=0, le=100, description="è¯¥é¡¹å¾—åˆ†ï¼ˆ0-100ï¼‰")
    message: str = Field(..., description="é—®é¢˜æè¿°æˆ–æˆåŠŸä¿¡æ¯")
    suggestion: str = Field(default="", description="æ”¹è¿›å»ºè®®")
    current_value: Optional[str] = Field(default=None, description="å½“å‰å€¼")
    expected_value: Optional[str] = Field(default=None, description="æœŸæœ›å€¼")

class AIAnalysisResult(BaseModel):
    """AIåˆ†æç»“æœ"""
    relevance_score: float = Field(..., ge=0, le=100, description="å†…å®¹ç›¸å…³æ€§ï¼ˆ0-100ï¼‰")
    depth_score: float = Field(..., ge=0, le=100, description="å†…å®¹æ·±åº¦ï¼ˆ0-100ï¼‰")
    readability_score: float = Field(..., ge=0, le=100, description="å¯è¯»æ€§ï¼ˆ0-100ï¼‰")
    overall_feedback: str = Field(default="", description="ç»¼åˆè¯„ä»·")
    improvement_suggestions: List[str] = Field(default_factory=list, description="æ”¹è¿›å»ºè®®åˆ—è¡¨")

class SEOReport(BaseModel):
    """å®Œæ•´SEOè¯Šæ–­æŠ¥å‘Š"""
    file_path: str
    total_score: float = Field(..., ge=0, le=100)

    # åˆ†é¡¹å¾—åˆ†
    metadata_score: float = Field(default=0, ge=0, le=30, description="å…ƒæ•°æ®å¾—åˆ†ï¼ˆæ»¡åˆ†30ï¼‰")
    structure_score: float = Field(default=0, ge=0, le=25, description="ç»“æ„å¾—åˆ†ï¼ˆæ»¡åˆ†25ï¼‰")
    keyword_score: float = Field(default=0, ge=0, le=20, description="å…³é”®è¯å¾—åˆ†ï¼ˆæ»¡åˆ†20ï¼‰")
    ai_score: float = Field(default=0, ge=0, le=25, description="AIè¯­ä¹‰å¾—åˆ†ï¼ˆæ»¡åˆ†25ï¼‰")

    # è¯¦ç»†è¯Šæ–­
    diagnostics: List[DiagnosticItem] = Field(default_factory=list)
    ai_analysis: Optional[AIAnalysisResult] = None

    # æå–çš„å…ƒæ•°æ®
    extracted_keywords: List[str] = Field(default_factory=list, description="è‡ªåŠ¨æå–çš„å…³é”®è¯")
    user_keywords: List[str] = Field(default_factory=list, description="ç”¨æˆ·æä¾›çš„å…³é”®è¯")

class ParsedMarkdown(BaseModel):
    """è§£æåçš„Markdownå†…å®¹"""
    frontmatter: Dict[str, any] = Field(default_factory=dict, description="YAML frontmatter")
    raw_content: str = Field(default="", description="å»é™¤frontmatterçš„Markdownæ­£æ–‡")
    html_content: str = Field(default="", description="è½¬æ¢åçš„HTML")
    title: str = Field(default="", description="ä»frontmatteræˆ–H1æå–çš„æ ‡é¢˜")
    description: str = Field(default="", description="ä»frontmatteræå–çš„æè¿°")
    h1_tags: List[str] = Field(default_factory=list, description="æ‰€æœ‰H1æ ‡ç­¾å†…å®¹")
    h2_tags: List[str] = Field(default_factory=list, description="æ‰€æœ‰H2æ ‡ç­¾å†…å®¹")
    images: List[Dict[str, str]] = Field(default_factory=list, description="å›¾ç‰‡åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{'src': '...', 'alt': '...'}]")
    links: List[Dict[str, str]] = Field(default_factory=list, description="é“¾æ¥åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{'href': '...', 'text': '...'}]")
    word_count: int = Field(default=0, description="æ­£æ–‡å­—æ•°")
```

**éªŒè¯æ£€æŸ¥ç‚¹**ï¼š
- [ ] è¿è¡Œ `python -c "from md_audit.models.data_models import SEOReport; print(SEOReport.model_json_schema())"` è¾“å‡ºåˆæ³•çš„JSON Schema
- [ ] åˆ›å»ºæµ‹è¯•å®ä¾‹ï¼š`report = SEOReport(file_path="test.md", total_score=85.5)` ä¸æŠ¥é”™
- [ ] æµ‹è¯•éªŒè¯ï¼š`DiagnosticItem(category="test", check_name="test", severity="critical", score=101)` åº”è¯¥æŠ›å‡ºValidationError

### Phase 2: é…ç½®ç³»ç»Ÿï¼ˆ30åˆ†é’Ÿï¼‰

**ç›®æ ‡**ï¼šå®ç°çµæ´»çš„é…ç½®ç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒJSONæ–‡ä»¶ + ç¯å¢ƒå˜é‡è¦†ç›–ã€‚

**å®ç°æ–‡ä»¶**ï¼š`md_audit/config.py`

**è®¾è®¡åŸåˆ™**ï¼š
- é…ç½®åŠ è½½ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ > é»˜è®¤é…ç½®
- æ‰€æœ‰é˜ˆå€¼éƒ½å¯é…ç½®ï¼Œä¸å…è®¸é­”æ³•æ•°å­—
- LLM APIå¯†é’¥å¿…é¡»é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼ˆå®‰å…¨è€ƒè™‘ï¼‰

**æ ¸å¿ƒä»£ç **ï¼š

```python
import os
import json
from dataclasses import dataclass, asdict
from typing import Optional
from pathlib import Path

@dataclass
class TitleRules:
    """æ ‡é¢˜è§„åˆ™é…ç½®"""
    min_length: int = 30
    max_length: int = 60
    weight: float = 15.0  # åœ¨30åˆ†å…ƒæ•°æ®ä¸­çš„æƒé‡

@dataclass
class DescriptionRules:
    """æè¿°è§„åˆ™é…ç½®"""
    min_length: int = 120
    max_length: int = 160
    weight: float = 15.0

@dataclass
class KeywordRules:
    """å…³é”®è¯è§„åˆ™é…ç½®"""
    min_density: float = 0.01  # æœ€å°å¯†åº¦1%
    max_density: float = 0.03  # æœ€å¤§å¯†åº¦3%
    max_auto_keywords: int = 5  # è‡ªåŠ¨æå–å…³é”®è¯æ•°é‡
    weight: float = 20.0

@dataclass
class ContentRules:
    """å†…å®¹ç»“æ„è§„åˆ™é…ç½®"""
    min_length: int = 500     # æœ€å°å­—æ•°
    min_h1_count: int = 1
    max_h1_count: int = 1
    min_image_alt_ratio: float = 0.8  # 80%çš„å›¾ç‰‡éœ€è¦alt
    structure_weight: float = 25.0

@dataclass
class MarkdownSEOConfig:
    """Markdown SEOé…ç½®ä¸»ç±»"""
    title: TitleRules = None
    description: DescriptionRules = None
    keywords: KeywordRules = None
    content: ContentRules = None

    # LLMé…ç½®
    llm_api_key: str = ""
    llm_base_url: str = "https://newapi.deepwisdom.ai/v1"
    llm_model: str = "gpt-4o"
    llm_timeout: int = 30
    llm_max_retries: int = 3
    enable_ai_analysis: bool = True

    def __post_init__(self):
        """åˆå§‹åŒ–é»˜è®¤å­é…ç½®"""
        if self.title is None:
            self.title = TitleRules()
        if self.description is None:
            self.description = DescriptionRules()
        if self.keywords is None:
            self.keywords = KeywordRules()
        if self.content is None:
            self.content = ContentRules()

    @classmethod
    def from_json(cls, json_path: str) -> 'MarkdownSEOConfig':
        """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # é€’å½’å®ä¾‹åŒ–åµŒå¥—dataclass
        config = cls(
            title=TitleRules(**data.get('title_rules', {})),
            description=DescriptionRules(**data.get('description_rules', {})),
            keywords=KeywordRules(**data.get('keyword_rules', {})),
            content=ContentRules(**data.get('content_rules', {})),
            llm_api_key=data.get('llm_api_key', ''),
            llm_base_url=data.get('llm_base_url', 'https://newapi.deepwisdom.ai/v1'),
            llm_model=data.get('llm_model', 'gpt-4o'),
            llm_timeout=data.get('llm_timeout', 30),
            llm_max_retries=data.get('llm_max_retries', 3),
            enable_ai_analysis=data.get('enable_ai_analysis', True),
        )

        # ç¯å¢ƒå˜é‡è¦†ç›–
        if os.getenv('MD_AUDIT_LLM_API_KEY'):
            config.llm_api_key = os.getenv('MD_AUDIT_LLM_API_KEY')
        if os.getenv('MD_AUDIT_LLM_MODEL'):
            config.llm_model = os.getenv('MD_AUDIT_LLM_MODEL')

        return config

    def to_json(self, json_path: str):
        """ä¿å­˜é…ç½®åˆ°JSONæ–‡ä»¶"""
        data = {
            'title_rules': asdict(self.title),
            'description_rules': asdict(self.description),
            'keyword_rules': asdict(self.keywords),
            'content_rules': asdict(self.content),
            'llm_api_key': '',  # ä¸ä¿å­˜æ•æ„Ÿä¿¡æ¯
            'llm_base_url': self.llm_base_url,
            'llm_model': self.llm_model,
            'llm_timeout': self.llm_timeout,
            'llm_max_retries': self.llm_max_retries,
            'enable_ai_analysis': self.enable_ai_analysis,
        }
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

def load_config(config_path: Optional[str] = None) -> MarkdownSEOConfig:
    """
    åŠ è½½é…ç½®ï¼ˆä¼˜å…ˆçº§ï¼šè‡ªå®šä¹‰è·¯å¾„ > é»˜è®¤è·¯å¾„ï¼‰

    Args:
        config_path: è‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å®ä¾‹
    """
    if config_path and Path(config_path).exists():
        return MarkdownSEOConfig.from_json(config_path)

    # é»˜è®¤é…ç½®è·¯å¾„
    default_path = Path(__file__).parent.parent / "config" / "default_config.json"
    if default_path.exists():
        return MarkdownSEOConfig.from_json(str(default_path))

    # ä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼
    return MarkdownSEOConfig()
```

**éªŒè¯æ£€æŸ¥ç‚¹**ï¼š
- [ ] åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶ï¼Œä¿®æ”¹ `min_length`ï¼ŒåŠ è½½åéªŒè¯å€¼æ˜¯å¦æ­£ç¡®
- [ ] è®¾ç½®ç¯å¢ƒå˜é‡ `export MD_AUDIT_LLM_API_KEY=test_key`ï¼ŒåŠ è½½é…ç½®åéªŒè¯æ˜¯å¦è¦†ç›–
- [ ] è¿è¡Œ `python -c "from md_audit.config import load_config; c=load_config(); print(c.llm_base_url)"` è¾“å‡ºæ­£ç¡®URL

### Phase 3: Markdownè§£æå™¨ï¼ˆ45åˆ†é’Ÿï¼‰

**ç›®æ ‡**ï¼šè§£æMarkdownæ–‡ä»¶ï¼Œæå–frontmatterã€æ ‡é¢˜ã€æè¿°ã€ç»“æ„å…ƒç´ ï¼Œå¹¶å®ç°æ™ºèƒ½å…³é”®è¯æå–ã€‚

**å®ç°æ–‡ä»¶**ï¼š`md_audit/parsers/markdown_parser.py`

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
1. è§£æYAML frontmatterï¼ˆæ ‡é¢˜ã€æè¿°ã€å…³é”®è¯ï¼‰
2. è½¬æ¢Markdownä¸ºHTMLï¼ˆç”¨äºç»“æ„åˆ†æï¼‰
3. æå–H1/H2æ ‡ç­¾ã€å›¾ç‰‡ã€é“¾æ¥
4. æ™ºèƒ½å…³é”®è¯æå–ï¼ˆåŸºäºn-gram + è´¨é‡è¿‡æ»¤ï¼‰

**å…³é”®è¯æå–é€»è¾‘**ï¼ˆå‚è€ƒ `analyzer.py:16-94, 185-235`ï¼‰ï¼š
- è®¡ç®—unigramsï¼ˆå•è¯ï¼‰ã€bigramsï¼ˆä¸¤è¯ç»„åˆï¼‰ã€trigramsï¼ˆä¸‰è¯ç»„åˆï¼‰çš„è¯é¢‘
- è¿‡æ»¤ä½è´¨é‡è¯ï¼šURLç‰‡æ®µã€HTML/CSSä»£ç ã€æŠ€æœ¯æœ¯è¯­ã€åœç”¨è¯
- æŒ‰è¯é¢‘æ’åºï¼Œè¿”å›Top N

**æ ¸å¿ƒä»£ç **ï¼š

```python
import re
from typing import List, Dict, Tuple
from pathlib import Path
import frontmatter
import markdown
from bs4 import BeautifulSoup
from md_audit.models.data_models import ParsedMarkdown

class MarkdownParser:
    """Markdownæ–‡ä»¶è§£æå™¨"""

    # å…³é”®è¯è´¨é‡è¿‡æ»¤è§„åˆ™ï¼ˆå‚è€ƒanalyzer.py:16-94ï¼‰
    LOW_QUALITY_PATTERNS = [
        r'^https?://',          # URL
        r'\.(com|org|net|io)',  # åŸŸå
        r'<[^>]+>',            # HTMLæ ‡ç­¾
        r'\{[^}]+\}',          # CSS/ä»£ç 
        r'^\d+$',              # çº¯æ•°å­—
        r'^[^a-zA-Z\u4e00-\u9fa5]+$',  # éå­—æ¯/æ±‰å­—
    ]

    # åœç”¨è¯ï¼ˆç®€åŒ–ç‰ˆï¼Œç”Ÿäº§ç¯å¢ƒéœ€è¦æ›´å®Œæ•´çš„åœç”¨è¯è¡¨ï¼‰
    STOP_WORDS = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
        'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
        'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€',
        'è¿™', 'é‚£', 'ä½ ', 'æˆ‘', 'ä»–', 'å¥¹', 'å®ƒ'
    }

    def __init__(self):
        self.md_parser = markdown.Markdown(extensions=['extra', 'codehilite'])

    def parse(self, file_path: str) -> ParsedMarkdown:
        """
        è§£æMarkdownæ–‡ä»¶

        Args:
            file_path: Markdownæ–‡ä»¶è·¯å¾„

        Returns:
            è§£æåçš„ç»“æ„åŒ–æ•°æ®
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            post = frontmatter.load(f)

        # æå–frontmatter
        fm = post.metadata
        raw_content = post.content

        # è½¬æ¢ä¸ºHTML
        html_content = self.md_parser.convert(raw_content)
        soup = BeautifulSoup(html_content, 'html.parser')

        # æå–æ ‡é¢˜ï¼ˆä¼˜å…ˆä»frontmatterï¼Œå¦åˆ™ä»ç¬¬ä¸€ä¸ªH1ï¼‰
        title = fm.get('title', '')
        if not title:
            h1 = soup.find('h1')
            title = h1.get_text(strip=True) if h1 else ''

        # æå–æè¿°
        description = fm.get('description', '') or fm.get('excerpt', '')

        # æå–H1å’ŒH2æ ‡ç­¾
        h1_tags = [h1.get_text(strip=True) for h1 in soup.find_all('h1')]
        h2_tags = [h2.get_text(strip=True) for h2 in soup.find_all('h2')]

        # æå–å›¾ç‰‡
        images = []
        for img in soup.find_all('img'):
            images.append({
                'src': img.get('src', ''),
                'alt': img.get('alt', '')
            })

        # æå–é“¾æ¥
        links = []
        for a in soup.find_all('a'):
            links.append({
                'href': a.get('href', ''),
                'text': a.get_text(strip=True)
            })

        # è®¡ç®—å­—æ•°ï¼ˆç§»é™¤HTMLæ ‡ç­¾åï¼‰
        text_content = soup.get_text()
        word_count = len(text_content.split())

        return ParsedMarkdown(
            frontmatter=fm,
            raw_content=raw_content,
            html_content=html_content,
            title=title,
            description=description,
            h1_tags=h1_tags,
            h2_tags=h2_tags,
            images=images,
            links=links,
            word_count=word_count
        )

    def extract_keywords(self, content: str, max_keywords: int = 5) -> List[str]:
        """
        è‡ªåŠ¨æå–å…³é”®è¯ï¼ˆåŸºäºn-gram + è´¨é‡è¿‡æ»¤ï¼‰

        å‚è€ƒï¼šanalyzer.py:185-235

        Args:
            content: æ–‡æœ¬å†…å®¹
            max_keywords: è¿”å›å…³é”®è¯æ•°é‡

        Returns:
            å…³é”®è¯åˆ—è¡¨ï¼ˆæŒ‰è¯é¢‘é™åºï¼‰
        """
        # æ¸…ç†æ–‡æœ¬
        text = self._clean_text(content)
        words = text.split()

        # è®¡ç®—n-gramè¯é¢‘
        keyword_freq: Dict[str, int] = {}

        # Unigrams
        for word in words:
            if self._is_quality_keyword(word):
                keyword_freq[word] = keyword_freq.get(word, 0) + 1

        # Bigrams
        for i in range(len(words) - 1):
            bigram = f"{words[i]} {words[i+1]}"
            if self._is_quality_keyword(bigram):
                keyword_freq[bigram] = keyword_freq.get(bigram, 0) + 1

        # Trigrams
        for i in range(len(words) - 2):
            trigram = f"{words[i]} {words[i+1]} {words[i+2]}"
            if self._is_quality_keyword(trigram):
                keyword_freq[trigram] = keyword_freq.get(trigram, 0) + 1

        # æŒ‰è¯é¢‘æ’åº
        sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)
        return [kw for kw, _ in sorted_keywords[:max_keywords]]

    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤ä»£ç å—ã€HTMLæ ‡ç­¾ç­‰ï¼‰"""
        # ç§»é™¤ä»£ç å—
        text = re.sub(r'```[\s\S]*?```', '', text)
        text = re.sub(r'`[^`]+`', '', text)
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # æ ‡å‡†åŒ–ç©ºç™½ç¬¦
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _is_quality_keyword(self, keyword: str) -> bool:
        """
        åˆ¤æ–­å…³é”®è¯è´¨é‡ï¼ˆå‚è€ƒanalyzer.py:16-94ï¼‰

        æ‹’ç»ï¼šURLç‰‡æ®µã€HTML/CSSä»£ç ã€çº¯æ•°å­—ã€åœç”¨è¯ã€è¿‡çŸ­/è¿‡é•¿
        """
        keyword = keyword.strip().lower()

        # é•¿åº¦æ£€æŸ¥
        if len(keyword) < 2 or len(keyword) > 50:
            return False

        # åœç”¨è¯æ£€æŸ¥
        if keyword in self.STOP_WORDS:
            return False

        # æ¨¡å¼åŒ¹é…æ£€æŸ¥
        for pattern in self.LOW_QUALITY_PATTERNS:
            if re.search(pattern, keyword):
                return False

        return True
```

**éªŒè¯æ£€æŸ¥ç‚¹**ï¼š
- [ ] åˆ›å»ºæµ‹è¯•Markdownæ–‡ä»¶ `tests/fixtures/sample.md`ï¼ˆåŒ…å«frontmatterã€H1ã€H2ã€å›¾ç‰‡ã€é“¾æ¥ï¼‰
- [ ] è¿è¡Œè§£æï¼š`parser = MarkdownParser(); result = parser.parse('tests/fixtures/sample.md')`
- [ ] éªŒè¯ `result.title` ä¸ä¸ºç©º
- [ ] éªŒè¯ `result.h1_tags` é•¿åº¦æ­£ç¡®
- [ ] æµ‹è¯•å…³é”®è¯æå–ï¼š`keywords = parser.extract_keywords("Python SEO optimization guide")`ï¼ŒéªŒè¯è¿”å›åˆç†çš„å…³é”®è¯

### Phase 4: è§„åˆ™å¼•æ“ï¼ˆ45åˆ†é’Ÿï¼‰

**ç›®æ ‡**ï¼šå®ç°åŸºäºé…ç½®çš„è§„åˆ™æ£€æŸ¥å¼•æ“ï¼ŒéªŒè¯å…ƒæ•°æ®ã€ç»“æ„ã€å…³é”®è¯ã€‚

**å®ç°æ–‡ä»¶**ï¼š`md_audit/engines/rules_engine.py`

**è¯„åˆ†é€»è¾‘**ï¼ˆå‚è€ƒPRDè¯„åˆ†è¡¨ï¼‰ï¼š
- **å…ƒæ•°æ®ï¼ˆ30åˆ†ï¼‰**ï¼šæ ‡é¢˜15åˆ† + æè¿°15åˆ†
- **ç»“æ„ï¼ˆ25åˆ†ï¼‰**ï¼šH1æ ‡ç­¾5åˆ† + å›¾ç‰‡alt 10åˆ† + å†…éƒ¨é“¾æ¥10åˆ†
- **å…³é”®è¯ï¼ˆ20åˆ†ï¼‰**ï¼šå¯†åº¦10åˆ† + ä½ç½®10åˆ†

**æ ¸å¿ƒä»£ç **ï¼š

```python
from typing import List
from md_audit.models.data_models import ParsedMarkdown, DiagnosticItem, SeverityLevel
from md_audit.config import MarkdownSEOConfig

class RulesEngine:
    """è§„åˆ™æ£€æŸ¥å¼•æ“"""

    def __init__(self, config: MarkdownSEOConfig):
        self.config = config

    def check_all(self, parsed: ParsedMarkdown, keywords: List[str]) -> tuple[float, List[DiagnosticItem]]:
        """
        æ‰§è¡Œæ‰€æœ‰è§„åˆ™æ£€æŸ¥

        Args:
            parsed: è§£æåçš„Markdownæ•°æ®
            keywords: å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨æˆ·æä¾›æˆ–è‡ªåŠ¨æå–ï¼‰

        Returns:
            (æ€»åˆ†, è¯Šæ–­é¡¹åˆ—è¡¨)
        """
        diagnostics: List[DiagnosticItem] = []

        # å…ƒæ•°æ®æ£€æŸ¥ï¼ˆ30åˆ†ï¼‰
        metadata_score = self._check_metadata(parsed, diagnostics)

        # ç»“æ„æ£€æŸ¥ï¼ˆ25åˆ†ï¼‰
        structure_score = self._check_structure(parsed, diagnostics)

        # å…³é”®è¯æ£€æŸ¥ï¼ˆ20åˆ†ï¼‰
        keyword_score = self._check_keywords(parsed, keywords, diagnostics)

        total_score = metadata_score + structure_score + keyword_score
        return total_score, diagnostics

    def _check_metadata(self, parsed: ParsedMarkdown, diagnostics: List[DiagnosticItem]) -> float:
        """æ£€æŸ¥å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ + æè¿°ï¼‰"""
        score = 0.0

        # æ ‡é¢˜æ£€æŸ¥ï¼ˆ15åˆ†ï¼‰
        title = parsed.title
        title_len = len(title)
        rules = self.config.title

        if not title:
            diagnostics.append(DiagnosticItem(
                category="metadata",
                check_name="title_exists",
                severity=SeverityLevel.CRITICAL,
                score=0,
                message="ç¼ºå°‘æ ‡é¢˜",
                suggestion="åœ¨frontmatterä¸­æ·»åŠ titleå­—æ®µæˆ–ä½¿ç”¨H1æ ‡ç­¾",
                current_value="æ— ",
                expected_value="å¿…é¡»å­˜åœ¨"
            ))
        elif title_len < rules.min_length:
            diagnostics.append(DiagnosticItem(
                category="metadata",
                check_name="title_length",
                severity=SeverityLevel.WARNING,
                score=7.5,  # 50%åˆ†æ•°
                message=f"æ ‡é¢˜è¿‡çŸ­ï¼ˆ{title_len}å­—ç¬¦ï¼‰",
                suggestion=f"æ ‡é¢˜å»ºè®®åœ¨{rules.min_length}-{rules.max_length}å­—ç¬¦ä¹‹é—´",
                current_value=str(title_len),
                expected_value=f"{rules.min_length}-{rules.max_length}"
            ))
            score += 7.5
        elif title_len > rules.max_length:
            diagnostics.append(DiagnosticItem(
                category="metadata",
                check_name="title_length",
                severity=SeverityLevel.WARNING,
                score=10,  # 67%åˆ†æ•°
                message=f"æ ‡é¢˜è¿‡é•¿ï¼ˆ{title_len}å­—ç¬¦ï¼‰",
                suggestion=f"æ ‡é¢˜å»ºè®®åœ¨{rules.min_length}-{rules.max_length}å­—ç¬¦ä¹‹é—´ï¼Œè¿‡é•¿å¯èƒ½è¢«æœç´¢å¼•æ“æˆªæ–­",
                current_value=str(title_len),
                expected_value=f"{rules.min_length}-{rules.max_length}"
            ))
            score += 10
        else:
            diagnostics.append(DiagnosticItem(
                category="metadata",
                check_name="title_length",
                severity=SeverityLevel.SUCCESS,
                score=15,
                message=f"æ ‡é¢˜é•¿åº¦åˆé€‚ï¼ˆ{title_len}å­—ç¬¦ï¼‰",
                current_value=str(title_len),
                expected_value=f"{rules.min_length}-{rules.max_length}"
            ))
            score += 15

        # æè¿°æ£€æŸ¥ï¼ˆ15åˆ†ï¼‰
        desc = parsed.description
        desc_len = len(desc)
        desc_rules = self.config.description

        if not desc:
            diagnostics.append(DiagnosticItem(
                category="metadata",
                check_name="description_exists",
                severity=SeverityLevel.CRITICAL,
                score=0,
                message="ç¼ºå°‘æè¿°",
                suggestion="åœ¨frontmatterä¸­æ·»åŠ descriptionå­—æ®µ",
                current_value="æ— ",
                expected_value="å¿…é¡»å­˜åœ¨"
            ))
        elif desc_len < desc_rules.min_length:
            diagnostics.append(DiagnosticItem(
                category="metadata",
                check_name="description_length",
                severity=SeverityLevel.WARNING,
                score=7.5,
                message=f"æè¿°è¿‡çŸ­ï¼ˆ{desc_len}å­—ç¬¦ï¼‰",
                suggestion=f"æè¿°å»ºè®®åœ¨{desc_rules.min_length}-{desc_rules.max_length}å­—ç¬¦ä¹‹é—´",
                current_value=str(desc_len),
                expected_value=f"{desc_rules.min_length}-{desc_rules.max_length}"
            ))
            score += 7.5
        elif desc_len > desc_rules.max_length:
            diagnostics.append(DiagnosticItem(
                category="metadata",
                check_name="description_length",
                severity=SeverityLevel.WARNING,
                score=10,
                message=f"æè¿°è¿‡é•¿ï¼ˆ{desc_len}å­—ç¬¦ï¼‰",
                suggestion=f"æè¿°å»ºè®®åœ¨{desc_rules.min_length}-{desc_rules.max_length}å­—ç¬¦ä¹‹é—´ï¼Œè¿‡é•¿ä¼šè¢«æœç´¢å¼•æ“æˆªæ–­",
                current_value=str(desc_len),
                expected_value=f"{desc_rules.min_length}-{desc_rules.max_length}"
            ))
            score += 10
        else:
            diagnostics.append(DiagnosticItem(
                category="metadata",
                check_name="description_length",
                severity=SeverityLevel.SUCCESS,
                score=15,
                message=f"æè¿°é•¿åº¦åˆé€‚ï¼ˆ{desc_len}å­—ç¬¦ï¼‰",
                current_value=str(desc_len),
                expected_value=f"{desc_rules.min_length}-{desc_rules.max_length}"
            ))
            score += 15

        return score

    def _check_structure(self, parsed: ParsedMarkdown, diagnostics: List[DiagnosticItem]) -> float:
        """æ£€æŸ¥ç»“æ„ï¼ˆH1 + å›¾ç‰‡alt + å†…éƒ¨é“¾æ¥ï¼‰"""
        score = 0.0
        rules = self.config.content

        # H1æ ‡ç­¾æ£€æŸ¥ï¼ˆ5åˆ†ï¼‰
        h1_count = len(parsed.h1_tags)
        if h1_count < rules.min_h1_count:
            diagnostics.append(DiagnosticItem(
                category="structure",
                check_name="h1_count",
                severity=SeverityLevel.CRITICAL,
                score=0,
                message=f"ç¼ºå°‘H1æ ‡ç­¾ï¼ˆå½“å‰{h1_count}ä¸ªï¼‰",
                suggestion="æ¯ä¸ªé¡µé¢åº”è¯¥æœ‰ä¸”ä»…æœ‰1ä¸ªH1æ ‡ç­¾",
                current_value=str(h1_count),
                expected_value="1"
            ))
        elif h1_count > rules.max_h1_count:
            diagnostics.append(DiagnosticItem(
                category="structure",
                check_name="h1_count",
                severity=SeverityLevel.WARNING,
                score=2.5,
                message=f"H1æ ‡ç­¾è¿‡å¤šï¼ˆå½“å‰{h1_count}ä¸ªï¼‰",
                suggestion="æ¯ä¸ªé¡µé¢åº”è¯¥æœ‰ä¸”ä»…æœ‰1ä¸ªH1æ ‡ç­¾ï¼Œå¤šä¸ªH1ä¼šåˆ†æ•£é¡µé¢ä¸»é¢˜",
                current_value=str(h1_count),
                expected_value="1"
            ))
            score += 2.5
        else:
            diagnostics.append(DiagnosticItem(
                category="structure",
                check_name="h1_count",
                severity=SeverityLevel.SUCCESS,
                score=5,
                message=f"H1æ ‡ç­¾æ•°é‡æ­£ç¡®ï¼ˆ{h1_count}ä¸ªï¼‰",
                current_value=str(h1_count),
                expected_value="1"
            ))
            score += 5

        # å›¾ç‰‡altæ£€æŸ¥ï¼ˆ10åˆ†ï¼‰
        total_images = len(parsed.images)
        images_with_alt = sum(1 for img in parsed.images if img['alt'])
        alt_ratio = images_with_alt / total_images if total_images > 0 else 1.0

        if total_images == 0:
            diagnostics.append(DiagnosticItem(
                category="structure",
                check_name="image_alt",
                severity=SeverityLevel.INFO,
                score=10,
                message="é¡µé¢æ— å›¾ç‰‡ï¼Œè·³è¿‡altæ£€æŸ¥",
            ))
            score += 10
        elif alt_ratio < rules.min_image_alt_ratio:
            alt_score = 10 * alt_ratio
            diagnostics.append(DiagnosticItem(
                category="structure",
                check_name="image_alt",
                severity=SeverityLevel.WARNING,
                score=alt_score,
                message=f"å›¾ç‰‡altè¦†ç›–ç‡ä¸è¶³ï¼ˆ{images_with_alt}/{total_images}ï¼‰",
                suggestion="æ‰€æœ‰å›¾ç‰‡éƒ½åº”è¯¥æ·»åŠ æè¿°æ€§çš„altå±æ€§ä»¥æå‡å¯è®¿é—®æ€§å’ŒSEO",
                current_value=f"{alt_ratio:.1%}",
                expected_value=f">={rules.min_image_alt_ratio:.0%}"
            ))
            score += alt_score
        else:
            diagnostics.append(DiagnosticItem(
                category="structure",
                check_name="image_alt",
                severity=SeverityLevel.SUCCESS,
                score=10,
                message=f"å›¾ç‰‡altè¦†ç›–ç‡è‰¯å¥½ï¼ˆ{images_with_alt}/{total_images}ï¼‰",
                current_value=f"{alt_ratio:.1%}"
            ))
            score += 10

        # å†…éƒ¨é“¾æ¥æ£€æŸ¥ï¼ˆ10åˆ†ï¼‰
        # ç®€åŒ–é€»è¾‘ï¼šåªæ£€æŸ¥æ˜¯å¦å­˜åœ¨é“¾æ¥
        link_count = len(parsed.links)
        if link_count == 0:
            diagnostics.append(DiagnosticItem(
                category="structure",
                check_name="internal_links",
                severity=SeverityLevel.WARNING,
                score=0,
                message="é¡µé¢æ— å†…éƒ¨é“¾æ¥",
                suggestion="æ·»åŠ ç›¸å…³æ–‡ç« çš„å†…éƒ¨é“¾æ¥å¯ä»¥æå‡ç”¨æˆ·ä½“éªŒå’ŒSEO"
            ))
        elif link_count < 3:
            diagnostics.append(DiagnosticItem(
                category="structure",
                check_name="internal_links",
                severity=SeverityLevel.INFO,
                score=5,
                message=f"å†…éƒ¨é“¾æ¥è¾ƒå°‘ï¼ˆ{link_count}ä¸ªï¼‰",
                suggestion="å»ºè®®å¢åŠ 2-5ä¸ªç›¸å…³æ–‡ç« é“¾æ¥",
                current_value=str(link_count),
                expected_value="2-5"
            ))
            score += 5
        else:
            diagnostics.append(DiagnosticItem(
                category="structure",
                check_name="internal_links",
                severity=SeverityLevel.SUCCESS,
                score=10,
                message=f"å†…éƒ¨é“¾æ¥æ•°é‡åˆç†ï¼ˆ{link_count}ä¸ªï¼‰",
                current_value=str(link_count)
            ))
            score += 10

        return score

    def _check_keywords(self, parsed: ParsedMarkdown, keywords: List[str], diagnostics: List[DiagnosticItem]) -> float:
        """æ£€æŸ¥å…³é”®è¯ï¼ˆå¯†åº¦ + ä½ç½®ï¼‰"""
        if not keywords:
            diagnostics.append(DiagnosticItem(
                category="keywords",
                check_name="keywords_exist",
                severity=SeverityLevel.INFO,
                score=10,  # æ²¡æœ‰å…³é”®è¯ç»™åŸºç¡€åˆ†
                message="æœªæä¾›å…³é”®è¯ï¼Œè·³è¿‡å…³é”®è¯æ£€æŸ¥"
            ))
            return 10.0

        score = 0.0
        content = parsed.raw_content.lower()
        total_words = len(content.split())

        # å…³é”®è¯å¯†åº¦æ£€æŸ¥ï¼ˆ10åˆ†ï¼‰
        keyword_occurrences = sum(content.count(kw.lower()) for kw in keywords)
        density = keyword_occurrences / total_words if total_words > 0 else 0

        rules = self.config.keywords
        if density < rules.min_density:
            diagnostics.append(DiagnosticItem(
                category="keywords",
                check_name="keyword_density",
                severity=SeverityLevel.WARNING,
                score=5,
                message=f"å…³é”®è¯å¯†åº¦è¿‡ä½ï¼ˆ{density:.2%}ï¼‰",
                suggestion=f"å»ºè®®å…³é”®è¯å¯†åº¦åœ¨{rules.min_density:.1%}-{rules.max_density:.1%}ä¹‹é—´",
                current_value=f"{density:.2%}",
                expected_value=f"{rules.min_density:.1%}-{rules.max_density:.1%}"
            ))
            score += 5
        elif density > rules.max_density:
            diagnostics.append(DiagnosticItem(
                category="keywords",
                check_name="keyword_density",
                severity=SeverityLevel.WARNING,
                score=7,
                message=f"å…³é”®è¯å¯†åº¦è¿‡é«˜ï¼ˆ{density:.2%}ï¼‰ï¼Œå¯èƒ½è¢«åˆ¤å®šä¸ºå…³é”®è¯å †ç Œ",
                suggestion=f"å»ºè®®å…³é”®è¯å¯†åº¦åœ¨{rules.min_density:.1%}-{rules.max_density:.1%}ä¹‹é—´",
                current_value=f"{density:.2%}",
                expected_value=f"{rules.min_density:.1%}-{rules.max_density:.1%}"
            ))
            score += 7
        else:
            diagnostics.append(DiagnosticItem(
                category="keywords",
                check_name="keyword_density",
                severity=SeverityLevel.SUCCESS,
                score=10,
                message=f"å…³é”®è¯å¯†åº¦åˆç†ï¼ˆ{density:.2%}ï¼‰",
                current_value=f"{density:.2%}"
            ))
            score += 10

        # å…³é”®è¯ä½ç½®æ£€æŸ¥ï¼ˆ10åˆ†ï¼‰
        # æ£€æŸ¥å…³é”®è¯æ˜¯å¦å‡ºç°åœ¨æ ‡é¢˜ã€æè¿°ã€H1
        kw_in_title = any(kw.lower() in parsed.title.lower() for kw in keywords)
        kw_in_desc = any(kw.lower() in parsed.description.lower() for kw in keywords)
        kw_in_h1 = any(kw.lower() in h1.lower() for h1 in parsed.h1_tags for kw in keywords)

        position_score = 0
        position_details = []

        if kw_in_title:
            position_score += 4
            position_details.append("æ ‡é¢˜âœ“")
        else:
            position_details.append("æ ‡é¢˜âœ—")

        if kw_in_desc:
            position_score += 3
            position_details.append("æè¿°âœ“")
        else:
            position_details.append("æè¿°âœ—")

        if kw_in_h1:
            position_score += 3
            position_details.append("H1âœ“")
        else:
            position_details.append("H1âœ—")

        severity = SeverityLevel.SUCCESS if position_score >= 7 else (
            SeverityLevel.WARNING if position_score >= 4 else SeverityLevel.CRITICAL
        )

        diagnostics.append(DiagnosticItem(
            category="keywords",
            check_name="keyword_position",
            severity=severity,
            score=position_score,
            message=f"å…³é”®è¯ä½ç½®è¦†ç›–ï¼š{' | '.join(position_details)}",
            suggestion="å…³é”®è¯åº”è¯¥å‡ºç°åœ¨æ ‡é¢˜ã€æè¿°å’ŒH1ä¸­ä»¥è·å¾—æœ€ä½³SEOæ•ˆæœ" if position_score < 10 else "",
            current_value=' | '.join(position_details)
        ))
        score += position_score

        return score
```

**éªŒè¯æ£€æŸ¥ç‚¹**ï¼š
- [ ] åˆ›å»ºæµ‹è¯•Markdownæ–‡ä»¶ï¼Œæ•…æ„è®¾ç½®æ ‡é¢˜è¿‡çŸ­ï¼ˆå¦‚"Test"ï¼‰
- [ ] è¿è¡Œè§„åˆ™å¼•æ“ï¼š`engine = RulesEngine(config); score, diags = engine.check_all(parsed, [])`
- [ ] éªŒè¯ `score < 100` ä¸”è¯Šæ–­åˆ—è¡¨åŒ…å«æ ‡é¢˜é•¿åº¦è­¦å‘Š
- [ ] æµ‹è¯•å…³é”®è¯æ£€æŸ¥ï¼šæä¾›å…³é”®è¯åˆ—è¡¨ `["Python", "SEO"]`ï¼ŒéªŒè¯å¯†åº¦å’Œä½ç½®æ£€æŸ¥æ­£å¸¸

### Phase 5: AIå¼•æ“ï¼ˆ45åˆ†é’Ÿï¼‰

**ç›®æ ‡**ï¼šé›†æˆOpenAI APIï¼Œå®ç°è¯­ä¹‰åˆ†æï¼ˆå†…å®¹ç›¸å…³æ€§ã€æ·±åº¦ã€å¯è¯»æ€§ï¼‰+ é‡è¯•æœºåˆ¶ + ä¼˜é›…é™çº§ã€‚

**å®ç°æ–‡ä»¶**ï¼š`md_audit/engines/ai_engine.py`

**å…³é”®è®¾è®¡**ï¼š
- ä½¿ç”¨ `openai` åº“è°ƒç”¨API
- 3æ¬¡é‡è¯•æœºåˆ¶ï¼ˆç½‘ç»œé”™è¯¯ã€APIé™æµï¼‰
- LLMè¿”å›JSONæ ¼å¼çš„è¯„åˆ†å’Œå»ºè®®
- å¤±è´¥æ—¶è¿”å› `None`ï¼ˆç”±ä¸»åˆ†æå™¨å¤„ç†é™çº§ï¼‰

**æ ¸å¿ƒä»£ç **ï¼š

```python
import os
import time
import json
from typing import Optional
from openai import OpenAI
from md_audit.models.data_models import AIAnalysisResult, ParsedMarkdown
from md_audit.config import MarkdownSEOConfig

class AIEngine:
    """AIè¯­ä¹‰åˆ†æå¼•æ“"""

    def __init__(self, config: MarkdownSEOConfig):
        self.config = config

        if not config.llm_api_key:
            raise ValueError("LLM API Keyæœªè®¾ç½®ï¼Œè¯·é€šè¿‡ç¯å¢ƒå˜é‡MD_AUDIT_LLM_API_KEYæä¾›")

        self.client = OpenAI(
            api_key=config.llm_api_key,
            base_url=config.llm_base_url,
            timeout=config.llm_timeout
        )

    def analyze(self, parsed: ParsedMarkdown, keywords: list[str]) -> Optional[AIAnalysisResult]:
        """
        AIè¯­ä¹‰åˆ†æï¼ˆå†…å®¹ç›¸å…³æ€§ã€æ·±åº¦ã€å¯è¯»æ€§ï¼‰

        Args:
            parsed: è§£æåçš„Markdownæ•°æ®
            keywords: å…³é”®è¯åˆ—è¡¨

        Returns:
            AIåˆ†æç»“æœï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if not self.config.enable_ai_analysis:
            return None

        # æ„é€ prompt
        keyword_str = "ã€".join(keywords) if keywords else "æœªæä¾›"
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªSEOä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹Markdownæ–‡ç« çš„è´¨é‡ã€‚

**æ–‡ç« æ ‡é¢˜**ï¼š{parsed.title}
**æ–‡ç« æè¿°**ï¼š{parsed.description}
**ç›®æ ‡å…³é”®è¯**ï¼š{keyword_str}
**å­—æ•°**ï¼š{parsed.word_count}

**æ–‡ç« å†…å®¹**ï¼ˆå‰1000å­—ï¼‰ï¼š
{parsed.raw_content[:1000]}

è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦è¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰ï¼š

1. **å†…å®¹ç›¸å…³æ€§ï¼ˆrelevance_scoreï¼‰**ï¼šæ–‡ç« å†…å®¹ä¸ç›®æ ‡å…³é”®è¯çš„åŒ¹é…åº¦
2. **å†…å®¹æ·±åº¦ï¼ˆdepth_scoreï¼‰**ï¼šå†…å®¹æ˜¯å¦æ·±å…¥ã€æ˜¯å¦æä¾›å®ç”¨ä»·å€¼
3. **å¯è¯»æ€§ï¼ˆreadability_scoreï¼‰**ï¼šç»“æ„æ˜¯å¦æ¸…æ™°ã€è¯­è¨€æ˜¯å¦æµç•…

åŒæ—¶æä¾›ï¼š
- **overall_feedback**ï¼š50å­—ä»¥å†…çš„ç»¼åˆè¯„ä»·
- **improvement_suggestions**ï¼š2-3æ¡å…·ä½“æ”¹è¿›å»ºè®®

**è¾“å‡ºæ ¼å¼**ï¼ˆJSONï¼‰ï¼š
{{
  "relevance_score": 85,
  "depth_score": 75,
  "readability_score": 90,
  "overall_feedback": "æ–‡ç« ä¸å…³é”®è¯ç›¸å…³æ€§å¼ºï¼Œä½†ç¼ºä¹å®æˆ˜æ¡ˆä¾‹",
  "improvement_suggestions": [
    "æ·»åŠ æ›´å¤šä»£ç ç¤ºä¾‹",
    "å¢åŠ å®é™…åº”ç”¨åœºæ™¯"
  ]
}}
"""

        # é‡è¯•æœºåˆ¶
        for attempt in range(self.config.llm_max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.config.llm_model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„SEOåˆ†æä¸“å®¶ï¼Œæ“…é•¿è¯„ä¼°å†…å®¹è´¨é‡ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,  # ä½æ¸©åº¦ä¿è¯è¾“å‡ºç¨³å®š
                    response_format={"type": "json_object"}  # å¼ºåˆ¶JSONè¾“å‡º
                )

                # è§£æå“åº”
                result_text = response.choices[0].message.content
                result_data = json.loads(result_text)

                # éªŒè¯å¹¶è¿”å›
                return AIAnalysisResult(
                    relevance_score=float(result_data.get('relevance_score', 0)),
                    depth_score=float(result_data.get('depth_score', 0)),
                    readability_score=float(result_data.get('readability_score', 0)),
                    overall_feedback=result_data.get('overall_feedback', ''),
                    improvement_suggestions=result_data.get('improvement_suggestions', [])
                )

            except json.JSONDecodeError as e:
                print(f"[è­¦å‘Š] AIè¿”å›ç»“æœè§£æå¤±è´¥ï¼ˆå°è¯• {attempt+1}/{self.config.llm_max_retries}ï¼‰ï¼š{e}")
                if attempt < self.config.llm_max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                continue

            except Exception as e:
                print(f"[è­¦å‘Š] AIåˆ†æå¤±è´¥ï¼ˆå°è¯• {attempt+1}/{self.config.llm_max_retries}ï¼‰ï¼š{e}")
                if attempt < self.config.llm_max_retries - 1:
                    time.sleep(2 ** attempt)
                continue

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        print("[é”™è¯¯] AIåˆ†æå¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå°†è·³è¿‡AIè¯„åˆ†")
        return None

    def calculate_ai_score(self, ai_result: Optional[AIAnalysisResult]) -> float:
        """
        è®¡ç®—AIè¯­ä¹‰å¾—åˆ†ï¼ˆæ»¡åˆ†25åˆ†ï¼‰

        Args:
            ai_result: AIåˆ†æç»“æœ

        Returns:
            AIå¾—åˆ†ï¼ˆ0-25ï¼‰
        """
        if not ai_result:
            return 0.0

        # åŠ æƒå¹³å‡ï¼šç›¸å…³æ€§40% + æ·±åº¦30% + å¯è¯»æ€§30%
        weighted_score = (
            ai_result.relevance_score * 0.4 +
            ai_result.depth_score * 0.3 +
            ai_result.readability_score * 0.3
        )

        # è½¬æ¢ä¸º25åˆ†åˆ¶
        return weighted_score * 0.25
```

**éªŒè¯æ£€æŸ¥ç‚¹**ï¼š
- [ ] è®¾ç½®ç¯å¢ƒå˜é‡ï¼š`export MD_AUDIT_LLM_API_KEY=sk-tVlvoM4GZwWVT7GQWWcU8aD7J0pGguWBGiPFd6l4uF4JVMRM`
- [ ] åˆ›å»ºæµ‹è¯•ï¼š`engine = AIEngine(config); result = engine.analyze(parsed, ["Python"])`
- [ ] éªŒè¯ `result` ä¸ä¸ºNone ä¸”åŒ…å« `relevance_score`, `depth_score`, `readability_score`
- [ ] æµ‹è¯•å¤±è´¥é™çº§ï¼šæ•…æ„è®¾ç½®é”™è¯¯çš„API keyï¼ŒéªŒè¯è¿”å›Noneè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸

### Phase 6: åˆ†æå™¨åè°ƒå™¨ + æŠ¥å‘Šç”Ÿæˆï¼ˆ45åˆ†é’Ÿï¼‰

**ç›®æ ‡**ï¼šæ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œåè°ƒè§„åˆ™å¼•æ“å’ŒAIå¼•æ“ï¼Œç”ŸæˆMarkdownè¯Šæ–­æŠ¥å‘Šã€‚

**å®ç°æ–‡ä»¶**ï¼š
- `md_audit/analyzer.py`ï¼šä¸»åˆ†æåè°ƒå™¨
- `md_audit/reporter.py`ï¼šMarkdownæŠ¥å‘Šç”Ÿæˆ

**åˆ†æå™¨æ ¸å¿ƒé€»è¾‘**ï¼š
1. è§£æMarkdownæ–‡ä»¶
2. æå–æˆ–ä½¿ç”¨ç”¨æˆ·æä¾›çš„å…³é”®è¯
3. è¿è¡Œè§„åˆ™å¼•æ“ï¼ˆè·å¾—75åˆ†ï¼‰
4. è¿è¡ŒAIå¼•æ“ï¼ˆè·å¾—25åˆ†ï¼‰
5. åˆå¹¶ç»“æœï¼Œç”ŸæˆæŠ¥å‘Š

**æ ¸å¿ƒä»£ç ï¼ˆanalyzer.pyï¼‰**ï¼š

```python
from md_audit.parsers.markdown_parser import MarkdownParser
from md_audit.engines.rules_engine import RulesEngine
from md_audit.engines.ai_engine import AIEngine
from md_audit.models.data_models import SEOReport
from md_audit.config import MarkdownSEOConfig

class MarkdownSEOAnalyzer:
    """Markdown SEOåˆ†æåè°ƒå™¨"""

    def __init__(self, config: MarkdownSEOConfig):
        self.config = config
        self.parser = MarkdownParser()
        self.rules_engine = RulesEngine(config)

        # AIå¼•æ“å¯é€‰ï¼ˆå¦‚æœé…ç½®ç¦ç”¨æˆ–API keyæœªè®¾ç½®ï¼‰
        self.ai_engine = None
        if config.enable_ai_analysis and config.llm_api_key:
            try:
                self.ai_engine = AIEngine(config)
            except ValueError as e:
                print(f"[è­¦å‘Š] AIå¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼š{e}")

    def analyze(self, file_path: str, user_keywords: list[str] = None) -> SEOReport:
        """
        åˆ†æMarkdownæ–‡ä»¶

        Args:
            file_path: Markdownæ–‡ä»¶è·¯å¾„
            user_keywords: ç”¨æˆ·æä¾›çš„å…³é”®è¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            å®Œæ•´çš„SEOè¯Šæ–­æŠ¥å‘Š
        """
        # Step 1: è§£æMarkdown
        parsed = self.parser.parse(file_path)

        # Step 2: ç¡®å®šå…³é”®è¯
        if user_keywords:
            keywords = user_keywords
            extracted = []
        else:
            # è‡ªåŠ¨æå–
            keywords = self.parser.extract_keywords(
                parsed.raw_content,
                max_keywords=self.config.keywords.max_auto_keywords
            )
            extracted = keywords

        # Step 3: è¿è¡Œè§„åˆ™å¼•æ“ï¼ˆæœ€å¤š75åˆ†ï¼‰
        rules_score, diagnostics = self.rules_engine.check_all(parsed, keywords)

        # Step 4: è¿è¡ŒAIå¼•æ“ï¼ˆæœ€å¤š25åˆ†ï¼‰
        ai_result = None
        ai_score = 0.0
        if self.ai_engine:
            ai_result = self.ai_engine.analyze(parsed, keywords)
            ai_score = self.ai_engine.calculate_ai_score(ai_result)

        # Step 5: è®¡ç®—æ€»åˆ†
        total_score = rules_score + ai_score

        # Step 6: åˆ†ç±»å¾—åˆ†ï¼ˆç”¨äºæŠ¥å‘Šå±•ç¤ºï¼‰
        metadata_score = sum(d.score for d in diagnostics if d.category == "metadata")
        structure_score = sum(d.score for d in diagnostics if d.category == "structure")
        keyword_score = sum(d.score for d in diagnostics if d.category == "keywords")

        # Step 7: æ„å»ºæŠ¥å‘Š
        return SEOReport(
            file_path=file_path,
            total_score=round(total_score, 1),
            metadata_score=round(metadata_score, 1),
            structure_score=round(structure_score, 1),
            keyword_score=round(keyword_score, 1),
            ai_score=round(ai_score, 1),
            diagnostics=diagnostics,
            ai_analysis=ai_result,
            extracted_keywords=extracted,
            user_keywords=user_keywords or []
        )
```

**æŠ¥å‘Šç”Ÿæˆå™¨æ ¸å¿ƒä»£ç ï¼ˆreporter.pyï¼‰**ï¼š

```python
from md_audit.models.data_models import SEOReport, SeverityLevel

class MarkdownReporter:
    """Markdownè¯Šæ–­æŠ¥å‘Šç”Ÿæˆå™¨"""

    SEVERITY_EMOJI = {
        SeverityLevel.CRITICAL: "ğŸ”´",
        SeverityLevel.WARNING: "ğŸŸ ",
        SeverityLevel.INFO: "ğŸŸ¡",
        SeverityLevel.SUCCESS: "ğŸŸ¢"
    }

    def generate(self, report: SEOReport) -> str:
        """
        ç”ŸæˆMarkdownæ ¼å¼çš„è¯Šæ–­æŠ¥å‘Š

        Args:
            report: SEOè¯Šæ–­æŠ¥å‘Šæ•°æ®

        Returns:
            Markdownæ ¼å¼çš„æŠ¥å‘Šæ–‡æœ¬
        """
        lines = []

        # æ ‡é¢˜
        lines.append(f"# SEOè¯Šæ–­æŠ¥å‘Š\n")
        lines.append(f"**æ–‡ä»¶**: `{report.file_path}`\n")
        lines.append(f"**æ€»åˆ†**: {report.total_score:.1f}/100\n")

        # åˆ†é¡¹å¾—åˆ†
        lines.append("## è¯„åˆ†è¯¦æƒ…\n")
        lines.append(f"- **å…ƒæ•°æ®**: {report.metadata_score:.1f}/30")
        lines.append(f"- **ç»“æ„**: {report.structure_score:.1f}/25")
        lines.append(f"- **å…³é”®è¯**: {report.keyword_score:.1f}/20")
        lines.append(f"- **AIè¯­ä¹‰**: {report.ai_score:.1f}/25\n")

        # å…³é”®è¯ä¿¡æ¯
        if report.user_keywords:
            lines.append(f"**ç›®æ ‡å…³é”®è¯**: {', '.join(report.user_keywords)}")
        if report.extracted_keywords:
            lines.append(f"**è‡ªåŠ¨æå–å…³é”®è¯**: {', '.join(report.extracted_keywords)}\n")

        # è¯Šæ–­è¯¦æƒ…ï¼ˆæŒ‰ç±»åˆ«åˆ†ç»„ï¼‰
        lines.append("## è¯Šæ–­è¯¦æƒ…\n")

        for category_name, category_key in [
            ("å…ƒæ•°æ®æ£€æŸ¥", "metadata"),
            ("ç»“æ„æ£€æŸ¥", "structure"),
            ("å…³é”®è¯æ£€æŸ¥", "keywords")
        ]:
            category_items = [d for d in report.diagnostics if d.category == category_key]
            if category_items:
                lines.append(f"### {category_name}\n")
                for item in category_items:
                    emoji = self.SEVERITY_EMOJI[item.severity]
                    lines.append(f"{emoji} **{item.check_name}** ({item.score:.1f}åˆ†)")
                    lines.append(f"   - {item.message}")
                    if item.suggestion:
                        lines.append(f"   - ğŸ’¡ å»ºè®®: {item.suggestion}")
                    if item.current_value and item.expected_value:
                        lines.append(f"   - å½“å‰å€¼: `{item.current_value}` | æœŸæœ›å€¼: `{item.expected_value}`")
                    lines.append("")

        # AIåˆ†æç»“æœ
        if report.ai_analysis:
            lines.append("## AIè¯­ä¹‰åˆ†æ\n")
            ai = report.ai_analysis
            lines.append(f"**ç»¼åˆè¯„ä»·**: {ai.overall_feedback}\n")
            lines.append(f"- å†…å®¹ç›¸å…³æ€§: {ai.relevance_score:.1f}/100")
            lines.append(f"- å†…å®¹æ·±åº¦: {ai.depth_score:.1f}/100")
            lines.append(f"- å¯è¯»æ€§: {ai.readability_score:.1f}/100\n")

            if ai.improvement_suggestions:
                lines.append("**æ”¹è¿›å»ºè®®**:\n")
                for i, suggestion in enumerate(ai.improvement_suggestions, 1):
                    lines.append(f"{i}. {suggestion}")
                lines.append("")

        # æ€»ç»“
        lines.append("## æ€»ç»“\n")
        if report.total_score >= 90:
            lines.append("âœ… SEOè´¨é‡ä¼˜ç§€ï¼Œç»§ç»­ä¿æŒï¼")
        elif report.total_score >= 70:
            lines.append("âš ï¸ SEOè´¨é‡è‰¯å¥½ï¼Œä½†ä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚")
        else:
            lines.append("âŒ SEOè´¨é‡éœ€è¦æ˜¾è‘—æ”¹è¿›ï¼Œè¯·é‡ç‚¹å…³æ³¨ä¸Šè¿°è¯Šæ–­é—®é¢˜ã€‚")

        return "\n".join(lines)
```

**éªŒè¯æ£€æŸ¥ç‚¹**ï¼š
- [ ] åˆ›å»ºå®Œæ•´çš„æµ‹è¯•Markdownæ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰å…ƒç´ ï¼‰
- [ ] è¿è¡Œå®Œæ•´åˆ†æï¼š`analyzer = MarkdownSEOAnalyzer(config); report = analyzer.analyze('test.md')`
- [ ] éªŒè¯ `report.total_score` åœ¨0-100ä¹‹é—´
- [ ] ç”ŸæˆæŠ¥å‘Šï¼š`reporter = MarkdownReporter(); md_report = reporter.generate(report)`
- [ ] éªŒè¯æŠ¥å‘ŠåŒ…å«æ‰€æœ‰sectionï¼ˆè¯„åˆ†è¯¦æƒ…ã€è¯Šæ–­è¯¦æƒ…ã€AIåˆ†æã€æ€»ç»“ï¼‰

### Phase 7: CLIå…¥å£ï¼ˆ30åˆ†é’Ÿï¼‰

**ç›®æ ‡**ï¼šå®ç°å‘½ä»¤è¡Œç•Œé¢ï¼Œæ”¯æŒå¤šç§ä½¿ç”¨åœºæ™¯ã€‚

**å®ç°æ–‡ä»¶**ï¼š`md_audit/main.py`

**æ”¯æŒçš„å‘½ä»¤**ï¼š
```bash
# åŸºç¡€ç”¨æ³•ï¼ˆè‡ªåŠ¨æå–å…³é”®è¯ï¼‰
md-audit analyze docs/article.md

# æŒ‡å®šå…³é”®è¯
md-audit analyze docs/article.md -k "Python" "SEO" "ä¼˜åŒ–"

# æŒ‡å®šé…ç½®æ–‡ä»¶
md-audit analyze docs/article.md --config custom_config.json

# è¾“å‡ºæŠ¥å‘Šåˆ°æ–‡ä»¶
md-audit analyze docs/article.md -o report.md

# ç¦ç”¨AIåˆ†æï¼ˆä»…è§„åˆ™æ£€æŸ¥ï¼‰
md-audit analyze docs/article.md --no-ai
```

**æ ¸å¿ƒä»£ç **ï¼š

```python
import argparse
from pathlib import Path
from md_audit.config import load_config
from md_audit.analyzer import MarkdownSEOAnalyzer
from md_audit.reporter import MarkdownReporter

def main():
    parser = argparse.ArgumentParser(
        description="Markdown SEOè¯Šæ–­å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  md-audit analyze article.md
  md-audit analyze article.md -k "Python" "SEO"
  md-audit analyze article.md --config custom.json -o report.md
        """
    )

    subparsers = parser.add_subparsers(dest='command', help='å­å‘½ä»¤')

    # analyzeå­å‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†æMarkdownæ–‡ä»¶')
    analyze_parser.add_argument('file', type=str, help='Markdownæ–‡ä»¶è·¯å¾„')
    analyze_parser.add_argument('-k', '--keywords', nargs='+', help='ç›®æ ‡å…³é”®è¯ï¼ˆå¯é€‰ï¼‰')
    analyze_parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    analyze_parser.add_argument('-o', '--output', type=str, help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    analyze_parser.add_argument('--no-ai', action='store_true', help='ç¦ç”¨AIåˆ†æ')

    args = parser.parse_args()

    if args.command == 'analyze':
        # åŠ è½½é…ç½®
        config = load_config(args.config)

        # è¦†ç›–AIå¼€å…³
        if args.no_ai:
            config.enable_ai_analysis = False

        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        file_path = Path(args.file)
        if not file_path.exists():
            print(f"é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {args.file}")
            return 1

        # è¿è¡Œåˆ†æ
        print(f"æ­£åœ¨åˆ†æ {args.file} ...")
        analyzer = MarkdownSEOAnalyzer(config)
        report = analyzer.analyze(str(file_path), user_keywords=args.keywords)

        # ç”ŸæˆæŠ¥å‘Š
        reporter = MarkdownReporter()
        report_md = reporter.generate(report)

        # è¾“å‡º
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report_md)
            print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ° {args.output}")
        else:
            print("\n" + report_md)

        # è¿”å›çŠ¶æ€ç ï¼ˆåŸºäºå¾—åˆ†ï¼‰
        return 0 if report.total_score >= 70 else 1

    else:
        parser.print_help()
        return 0

if __name__ == '__main__':
    exit(main())
```

**setup.pyé…ç½®**ï¼ˆæ”¯æŒ `pip install` å®‰è£…ï¼‰ï¼š

```python
from setuptools import setup, find_packages

setup(
    name='md-audit',
    version='1.0.0',
    description='Markdown SEOè¯Šæ–­å·¥å…·',
    packages=find_packages(),
    install_requires=[
        'pydantic>=2.0.0',
        'python-frontmatter>=1.0.0',
        'markdown>=3.4.0',
        'beautifulsoup4>=4.12.0',
        'openai>=1.0.0',
        'pyyaml>=6.0',
    ],
    entry_points={
        'console_scripts': [
            'md-audit=md_audit.main:main',
        ],
    },
    python_requires='>=3.8',
)
```

**éªŒè¯æ£€æŸ¥ç‚¹**ï¼š
- [ ] è¿è¡Œ `pip install -e .` å®‰è£…åŒ…
- [ ] æµ‹è¯•å‘½ä»¤ï¼š`md-audit analyze tests/fixtures/sample.md`
- [ ] éªŒè¯è¾“å‡ºåŒ…å«å®Œæ•´æŠ¥å‘Š
- [ ] æµ‹è¯•å‚æ•°ï¼š`md-audit analyze sample.md -k "test" -o output.md --no-ai`
- [ ] éªŒè¯ `output.md` æ–‡ä»¶æ­£ç¡®ç”Ÿæˆ

## æµ‹è¯•è¦æ±‚

æ¯ä¸ªé˜¶æ®µå®Œæˆåå¿…é¡»éªŒè¯ï¼š

1. **å•å…ƒæµ‹è¯•è¦†ç›–**ï¼š
   - `tests/unit/test_parser.py`ï¼šæµ‹è¯•å…³é”®è¯æå–ã€Markdownè§£æ
   - `tests/unit/test_rules_engine.py`ï¼šæµ‹è¯•æ‰€æœ‰è§„åˆ™æ£€æŸ¥é€»è¾‘
   - `tests/unit/test_ai_engine.py`ï¼šæµ‹è¯•AIè°ƒç”¨å’Œé™çº§é€»è¾‘
   - `tests/unit/test_analyzer.py`ï¼šæµ‹è¯•ç«¯åˆ°ç«¯åˆ†ææµç¨‹

2. **é›†æˆæµ‹è¯•**ï¼š
   - åˆ›å»º `tests/fixtures/` ç›®å½•ï¼ŒåŒ…å«å¤šä¸ªæµ‹è¯•Markdownæ–‡ä»¶ï¼š
     - `good_example.md`ï¼šé«˜è´¨é‡æ–‡ç« ï¼ˆæœŸæœ›å¾—åˆ†>85ï¼‰
     - `bad_example.md`ï¼šä½è´¨é‡æ–‡ç« ï¼ˆæœŸæœ›å¾—åˆ†<50ï¼‰
     - `medium_example.md`ï¼šä¸­ç­‰è´¨é‡ï¼ˆæœŸæœ›å¾—åˆ†60-80ï¼‰

3. **æ‰‹åŠ¨éªŒè¯**ï¼š
   - è¿è¡Œ `md-audit analyze` å¯¹å®é™…åšå®¢æ–‡ç« è¿›è¡Œåˆ†æ
   - éªŒè¯AIåˆ†æç»“æœæ˜¯å¦åˆç†
   - éªŒè¯MarkdownæŠ¥å‘Šæ ¼å¼æ˜¯å¦æ¸…æ™°

## å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜1ï¼šLLMè°ƒç”¨å¤±è´¥

**ç—‡çŠ¶**ï¼šAIåˆ†æå§‹ç»ˆè¿”å›None

**æ’æŸ¥æ­¥éª¤**ï¼š
1. éªŒè¯API keyæ˜¯å¦æ­£ç¡®ï¼š`echo $MD_AUDIT_LLM_API_KEY`
2. éªŒè¯API endpointå¯è®¿é—®ï¼š`curl https://newapi.deepwisdom.ai/v1/models`
3. æ£€æŸ¥ç½‘ç»œä»£ç†è®¾ç½®
4. æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼ˆåœ¨ `ai_engine.py` ä¸­æ·»åŠ  `print` è¯­å¥ï¼‰

### é—®é¢˜2ï¼šå…³é”®è¯æå–è´¨é‡å·®

**ç—‡çŠ¶**ï¼šè‡ªåŠ¨æå–çš„å…³é”®è¯å…¨æ˜¯æ— æ„ä¹‰çš„è¯

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥ `LOW_QUALITY_PATTERNS` æ˜¯å¦è¦†ç›–å¸¸è§å™ªå£°
2. æ‰©å……åœç”¨è¯è¡¨ï¼ˆå‚è€ƒjiebaåˆ†è¯çš„åœç”¨è¯è¡¨ï¼‰
3. è°ƒæ•´n-gramæƒé‡ï¼ˆä¼˜å…ˆbigramå’Œtrigramï¼‰

### é—®é¢˜3ï¼šè¯„åˆ†ä¸åˆç†

**ç—‡çŠ¶**ï¼šæ˜æ˜¾é«˜è´¨é‡æ–‡ç« å¾—åˆ†å¾ˆä½

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥é…ç½®æ–‡ä»¶çš„é˜ˆå€¼æ˜¯å¦åˆç†ï¼ˆå¦‚æ ‡é¢˜é•¿åº¦èŒƒå›´ï¼‰
2. éªŒè¯è§„åˆ™å¼•æ“çš„æƒé‡åˆ†é…
3. å¯¹æ¯”è¯Šæ–­è¯¦æƒ…ï¼Œæ‰¾å‡ºæ‰£åˆ†é¡¹

### é—®é¢˜4ï¼šä¸­æ–‡åˆ†è¯ä¸å‡†ç¡®

**ç—‡çŠ¶**ï¼šä¸­æ–‡å…³é”®è¯æå–æ•ˆæœå·®

**è§£å†³æ–¹æ¡ˆ**ï¼š
é›†æˆjiebaåˆ†è¯åº“ï¼š
```python
import jieba

def extract_keywords_chinese(self, content: str, max_keywords: int = 5) -> List[str]:
    words = jieba.cut(content)
    # åç»­é€»è¾‘åŒè‹±æ–‡ç‰ˆ
```

## ä¼˜åŒ–å»ºè®®ï¼ˆå¯é€‰ï¼Œä¸åœ¨MVPèŒƒå›´ï¼‰

1. **ç¼“å­˜æœºåˆ¶**ï¼šç¼“å­˜LLMåˆ†æç»“æœï¼Œé¿å…é‡å¤è°ƒç”¨
2. **æ‰¹é‡åˆ†æ**ï¼šæ”¯æŒ `md-audit analyze docs/*.md` æ‰¹é‡å¤„ç†
3. **Webç•Œé¢**ï¼šä½¿ç”¨Streamlitåˆ›å»ºäº¤äº’å¼ç•Œé¢
4. **CI/CDé›†æˆ**ï¼šæä¾›GitHub Actionï¼Œè‡ªåŠ¨æ£€æŸ¥PRä¸­çš„Markdownæ–‡ä»¶
5. **è‡ªå®šä¹‰è§„åˆ™**ï¼šå…è®¸ç”¨æˆ·ç¼–å†™Pythonæ’ä»¶æ‰©å±•è§„åˆ™å¼•æ“

## äº¤ä»˜æ¸…å•

å®Œæˆåï¼Œé¡¹ç›®åº”åŒ…å«ï¼š

- [x] å®Œæ•´çš„ä»£ç å®ç°ï¼ˆ7ä¸ªæ¨¡å—ï¼‰
- [x] å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
- [x] `README.md`ï¼ˆåŒ…å«å®‰è£…ã€ä½¿ç”¨ã€é…ç½®è¯´æ˜ï¼‰
- [x] `requirements.txt` å’Œ `setup.py`
- [x] `config/default_config.json` é»˜è®¤é…ç½®
- [x] `tests/fixtures/` æµ‹è¯•ç”¨Markdownæ–‡ä»¶
- [x] ç¤ºä¾‹è¾“å‡ºæŠ¥å‘Š `examples/sample_report.md`

## éªŒæ”¶æ ‡å‡†

1. è¿è¡Œ `md-audit analyze tests/fixtures/good_example.md` å¾—åˆ†>85
2. è¿è¡Œ `md-audit analyze tests/fixtures/bad_example.md` å¾—åˆ†<50
3. AIåˆ†æåŠŸèƒ½æ­£å¸¸ï¼ˆæˆ–ä¼˜é›…é™çº§åˆ°è§„åˆ™å¼•æ“ï¼‰
4. æŠ¥å‘Šæ ¼å¼æ¸…æ™°ï¼ŒåŒ…å«å…·ä½“æ”¹è¿›å»ºè®®
5. æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡

## æœ€åæé†’

- **ä¸è¦ç…§æ¬å‚è€ƒä»£ç **ï¼Œç†è§£å…¶è®¾è®¡æ¨¡å¼åé‡æ–°å®ç°
- **ä¼˜å…ˆä¿è¯æ ¸å¿ƒåŠŸèƒ½**ï¼Œä¸è¦è¿‡åº¦ä¼˜åŒ–è¾¹ç¼˜æƒ…å†µ
- **æµ‹è¯•é©±åŠ¨å¼€å‘**ï¼Œå…ˆå†™æµ‹è¯•å†å®ç°åŠŸèƒ½
- **é‡åˆ°é—®é¢˜å…ˆæŸ¥æ—¥å¿—**ï¼Œæ·»åŠ è¶³å¤Ÿçš„ `print` æˆ– `logging` è¯­å¥
- **API keyå®‰å…¨**ï¼Œæ°¸è¿œä¸è¦ç¡¬ç¼–ç åœ¨ä»£ç ä¸­

å¼€å§‹ç¼–ç å§ï¼æœ‰é—®é¢˜éšæ—¶å‚è€ƒPRDå’ŒæŠ€æœ¯è®¾è®¡æ–‡æ¡£ã€‚
