# æŠ€æœ¯è®¾è®¡æ–‡æ¡£ (Technical Design Document)
# Markdown SEO è¯Šæ–­ Agent

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2025-11-27
**æŠ€æœ¯è´Ÿè´£äºº**: Claude Code (Tech Architect)
**å¼€å‘å›¢é˜Ÿ**: Codex

---

## 1. ç³»ç»Ÿæ¶æ„

### 1.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLI Layer                           â”‚
â”‚                    (main.py + argparse)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Core Analyzer                            â”‚
â”‚               (MarkdownSEOAnalyzer)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Markdown Parser  â†’ 2. Rules Engine  â†’ 3. AI Engine  â”‚
â”‚  â”‚  4. Score Calculator â†’ 5. Report Generator           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                    â”‚
          â–¼                  â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Markdown Parser â”‚  â”‚ Rules Engine â”‚  â”‚    AI Engine       â”‚
â”‚  (parsers.py)    â”‚  â”‚(rules_engine.â”‚  â”‚   (ai_engine.py)   â”‚
â”‚                  â”‚  â”‚      py)     â”‚  â”‚                    â”‚
â”‚ - frontmatter    â”‚  â”‚              â”‚  â”‚ - OpenAI Client    â”‚
â”‚ - markdown       â”‚  â”‚ - META_*     â”‚  â”‚ - Prompt Builder   â”‚
â”‚ - BeautifulSoup  â”‚  â”‚ - STRUC_*    â”‚  â”‚ - Response Parser  â”‚
â”‚                  â”‚  â”‚ - KEY_*      â”‚  â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Data Models       â”‚
                   â”‚   (models.py)       â”‚
                   â”‚                     â”‚
                   â”‚ - SEOReport         â”‚
                   â”‚ - Issue             â”‚
                   â”‚ - Suggestion        â”‚
                   â”‚ - RuleCheckResult   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Configuration     â”‚
                   â”‚   (config.py)       â”‚
                   â”‚                     â”‚
                   â”‚ - MarkdownSEOConfig â”‚
                   â”‚ - JSON loader       â”‚
                   â”‚ - Env var support   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ¨¡å—èŒè´£

| æ¨¡å— | æ–‡ä»¶ | èŒè´£ | ä¾èµ– |
|------|------|------|------|
| **CLIå±‚** | `main.py` | å‘½ä»¤è¡Œå…¥å£ï¼Œå‚æ•°è§£æ | argparse |
| **æ ¸å¿ƒåˆ†æå™¨** | `analyzer.py` | åè°ƒè§£æã€è§„åˆ™æ£€æŸ¥ã€AIåˆ†æ | æ‰€æœ‰æ¨¡å— |
| **Markdownè§£æå™¨** | `parsers.py` | è§£æFrontmatterå’ŒMarkdown | frontmatter, markdown, BeautifulSoup |
| **è§„åˆ™å¼•æ“** | `rules_engine.py` | æ‰§è¡Œé™æ€è§„åˆ™æ£€æŸ¥ | config.py, models.py |
| **AIå¼•æ“** | `ai_engine.py` | LLMè¯­ä¹‰åˆ†æ | openai, config.py |
| **æŠ¥å‘Šç”Ÿæˆå™¨** | `reporter.py` | ç”ŸæˆMarkdownæŠ¥å‘Š | models.py |
| **æ•°æ®æ¨¡å‹** | `models.py` | Pydanticæ•°æ®æ¨¡å‹å®šä¹‰ | pydantic |
| **é…ç½®ç³»ç»Ÿ** | `config.py` | è§„åˆ™é…ç½®ç®¡ç† | dataclasses, json |

---

## 2. ç›®å½•ç»“æ„

```
MD_Audit/
â”œâ”€â”€ md_seo_agent/           # ä¸»åŒ…ç›®å½•
â”‚   â”œâ”€â”€ __init__.py         # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ models.py           # Pydanticæ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ config.py           # é…ç½®ç³»ç»Ÿ
â”‚   â”œâ”€â”€ parsers.py          # Markdownè§£æå™¨
â”‚   â”œâ”€â”€ rules_engine.py     # è§„åˆ™æ£€æŸ¥å¼•æ“
â”‚   â”œâ”€â”€ ai_engine.py        # AIè¯­ä¹‰åˆ†æå¼•æ“
â”‚   â”œâ”€â”€ reporter.py         # æŠ¥å‘Šç”Ÿæˆå™¨
â”‚   â””â”€â”€ analyzer.py         # æ ¸å¿ƒåˆ†æå™¨
â”œâ”€â”€ config/                 # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â””â”€â”€ seo_rules.json      # é»˜è®¤SEOè§„åˆ™é…ç½®
â”œâ”€â”€ tests/                  # æµ‹è¯•ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_parsers.py
â”‚   â”œâ”€â”€ test_rules_engine.py
â”‚   â”œâ”€â”€ test_ai_engine.py
â”‚   â””â”€â”€ test_analyzer.py
â”œâ”€â”€ examples/               # ç¤ºä¾‹æ–‡ä»¶
â”‚   â”œâ”€â”€ sample_good.md      # é«˜åˆ†ç¤ºä¾‹
â”‚   â”œâ”€â”€ sample_bad.md       # ä½åˆ†ç¤ºä¾‹
â”‚   â””â”€â”€ sample_report.md    # æŠ¥å‘Šç¤ºä¾‹
â”œâ”€â”€ docs/                   # æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ PRD.md              # äº§å“éœ€æ±‚æ–‡æ¡£
â”‚   â”œâ”€â”€ TECH_DESIGN.md      # æœ¬æ–‡æ¡£
â”‚   â”œâ”€â”€ CODEX_PROMPT.md     # Codexå¼€å‘æŒ‡ä»¤
â”‚   â””â”€â”€ TEST_PLAN.md        # æµ‹è¯•è®¡åˆ’
â”œâ”€â”€ main.py                 # CLIå…¥å£
â”œâ”€â”€ requirements.txt        # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ .env.example            # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â””â”€â”€ README.md               # é¡¹ç›®è¯´æ˜
```

---

## 3. æ•°æ®æ¨¡å‹è®¾è®¡

### 3.1 models.py å®Œæ•´å®šä¹‰

```python
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
from datetime import datetime

class Issue(BaseModel):
    """è¯Šæ–­é—®é¢˜"""
    id: str = Field(..., description="é—®é¢˜IDï¼Œå¦‚META_01")
    severity: str = Field(..., description="ä¸¥é‡ç¨‹åº¦: critical, high, medium, low")
    category: str = Field(..., description="åˆ†ç±»: metadata, structure, keyword, content")
    message: str = Field(..., description="é—®é¢˜æè¿°")
    current_value: Optional[str] = Field(None, description="å½“å‰å€¼")
    expected_value: Optional[str] = Field(None, description="æœŸæœ›å€¼")
    fix_example: Optional[str] = Field(None, description="ä¿®å¤ç¤ºä¾‹ä»£ç ")
    location: Optional[str] = Field(None, description="é—®é¢˜ä½ç½®ï¼Œå¦‚'Frontmatterç¬¬3è¡Œ'")

    class Config:
        json_schema_extra = {
            "example": {
                "id": "META_01",
                "severity": "critical",
                "category": "metadata",
                "message": "Titleç¼ºå¤±",
                "current_value": "",
                "expected_value": "30-60å­—ç¬¦çš„æ ‡é¢˜",
                "fix_example": "---\ntitle: \"Markdown SEOæœ€ä½³å®è·µ\"\n---",
                "location": "Frontmatter"
            }
        }

class Suggestion(BaseModel):
    """ä¼˜åŒ–å»ºè®®"""
    priority: str = Field(..., description="ä¼˜å…ˆçº§: critical, high, medium, low")
    category: str = Field(..., description="åˆ†ç±»")
    recommendation: str = Field(..., description="å»ºè®®å†…å®¹")
    rationale: Optional[str] = Field(None, description="å»ºè®®ç†ç”±")
    example: Optional[str] = Field(None, description="ç¤ºä¾‹")

class ScoreBreakdown(BaseModel):
    """åˆ†é¡¹å¾—åˆ†"""
    dimension: str = Field(..., description="è¯„åˆ†ç»´åº¦")
    score: float = Field(..., ge=0, le=100, description="å¾—åˆ†")
    weight: float = Field(..., ge=0, le=1, description="æƒé‡")
    weighted_score: float = Field(..., ge=0, le=100, description="åŠ æƒå¾—åˆ†")
    status: str = Field(..., description="çŠ¶æ€: critical/high/medium/low")

    @property
    def emoji(self) -> str:
        """æ ¹æ®åˆ†æ•°è¿”å›emoji"""
        if self.score < 40:
            return "ğŸ”´"
        elif self.score < 60:
            return "ğŸŸ "
        elif self.score < 80:
            return "ğŸŸ¡"
        else:
            return "ğŸŸ¢"

class AIAnalysisResult(BaseModel):
    """AIåˆ†æç»“æœ"""
    content_depth_score: float = Field(..., ge=0, le=10, description="å†…å®¹æ·±åº¦è¯„åˆ†")
    readability_score: float = Field(..., ge=0, le=10, description="å¯è¯»æ€§è¯„åˆ†")
    keyword_relevance_score: Optional[float] = Field(None, ge=0, le=10, description="å…³é”®è¯ç›¸å…³æ€§è¯„åˆ†")
    recommendations: List[str] = Field(default_factory=list, description="AIå»ºè®®åˆ—è¡¨")
    analysis_details: Optional[Dict[str, Any]] = Field(None, description="è¯¦ç»†åˆ†æ")

class RuleCheckResult(BaseModel):
    """è§„åˆ™æ£€æŸ¥ç»“æœ"""
    rule_id: str
    passed: bool
    score: float = Field(ge=0, le=100)
    severity: str
    message: str
    details: Optional[Dict[str, Any]] = None

class SEOReport(BaseModel):
    """SEOè¯Šæ–­æŠ¥å‘Šä¸»ä½“"""
    file_path: str = Field(..., description="åˆ†æçš„æ–‡ä»¶è·¯å¾„")
    analysis_time: datetime = Field(default_factory=datetime.now, description="åˆ†ææ—¶é—´")
    total_score: float = Field(..., ge=0, le=100, description="æ€»åˆ†")

    # åˆ†é¡¹å¾—åˆ†
    score_breakdown: List[ScoreBreakdown] = Field(..., description="å„ç»´åº¦å¾—åˆ†è¯¦æƒ…")

    # é—®é¢˜åˆ—è¡¨
    critical_issues: List[Issue] = Field(default_factory=list)
    high_priority_issues: List[Issue] = Field(default_factory=list)
    medium_priority_issues: List[Issue] = Field(default_factory=list)
    low_priority_issues: List[Issue] = Field(default_factory=list)

    # å»ºè®®åˆ—è¡¨
    suggestions: List[Suggestion] = Field(default_factory=list)

    # AIåˆ†æç»“æœ
    ai_analysis: Optional[AIAnalysisResult] = Field(None, description="AIè¯­ä¹‰åˆ†æç»“æœ")

    # å…³é”®è¯ä¿¡æ¯
    keywords_analyzed: List[str] = Field(default_factory=list, description="åˆ†æçš„å…³é”®è¯åˆ—è¡¨")
    auto_extracted_keywords: bool = Field(False, description="æ˜¯å¦è‡ªåŠ¨æå–å…³é”®è¯")

    @property
    def emoji_badge(self) -> str:
        """æ€»åˆ†å¯¹åº”çš„emojiå¾½ç« """
        if self.total_score < 40:
            return "ğŸ”´"
        elif self.total_score < 60:
            return "ğŸŸ "
        elif self.total_score < 80:
            return "ğŸŸ¡"
        else:
            return "ğŸŸ¢"

    class Config:
        json_schema_extra = {
            "example": {
                "file_path": "examples/sample.md",
                "total_score": 68.5,
                "score_breakdown": [
                    {"dimension": "å…ƒæ•°æ®", "score": 20, "weight": 0.3, "weighted_score": 6, "status": "high"},
                    {"dimension": "ç»“æ„", "score": 18, "weight": 0.25, "weighted_score": 4.5, "status": "medium"},
                    {"dimension": "å…³é”®è¯", "score": 15, "weight": 0.2, "weighted_score": 3, "status": "medium"},
                    {"dimension": "AIè¯­ä¹‰", "score": 15, "weight": 0.25, "weighted_score": 3.75, "status": "medium"}
                ]
            }
        }
```

---

## 4. æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 4.1 é…ç½®ç³»ç»Ÿ (config.py)

**å‚è€ƒ**: `/mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/seo_rules_config.py`

```python
import os
import json
import logging
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Optional

logger = logging.getLogger(__name__)

@dataclass
class TitleRules:
    """Titleè§„åˆ™é…ç½®"""
    min_length: int = 30
    max_length: int = 60
    ideal_min: int = 50
    ideal_max: int = 60
    critical_threshold: int = 30
    warning_threshold: int = 60

    def get_priority_for_length(self, length: int) -> str:
        """æ ¹æ®é•¿åº¦è¿”å›ä¼˜å…ˆçº§"""
        if length == 0:
            return "critical"
        elif length < self.critical_threshold:
            return "high"
        elif length > self.warning_threshold:
            return "high"
        elif length < self.ideal_min:
            return "medium"
        else:
            return "low"

@dataclass
class DescriptionRules:
    """Descriptionè§„åˆ™é…ç½®"""
    min_length: int = 120
    max_length: int = 160
    ideal_min: int = 150
    ideal_max: int = 160
    critical_threshold: int = 120

    def get_priority_for_length(self, length: int) -> str:
        if length == 0:
            return "high"
        elif length < self.critical_threshold:
            return "medium"
        elif length > self.max_length:
            return "medium"
        else:
            return "low"

@dataclass
class KeywordRules:
    """å…³é”®è¯è§„åˆ™é…ç½®"""
    min_density: float = 0.01  # 1%
    max_density: float = 0.025  # 2.5%
    ideal_density: float = 0.015  # 1.5%

    def get_priority_for_density(self, density: float) -> str:
        if density < self.min_density:
            return "medium"
        elif density > self.max_density:
            return "high"  # å…³é”®è¯å †ç Œ
        else:
            return "low"

@dataclass
class ContentRules:
    """å†…å®¹è§„åˆ™é…ç½®"""
    min_word_count: int = 300
    recommended_min: int = 500
    ideal_word_count: int = 1000

    def get_priority_for_word_count(self, word_count: int) -> str:
        if word_count < self.min_word_count:
            return "high"
        elif word_count < self.recommended_min:
            return "medium"
        else:
            return "low"

@dataclass
class MarkdownSEOConfig:
    """Markdown SEOé…ç½®ä¸»ç±»"""
    # è§„åˆ™é…ç½®
    title: TitleRules = None
    description: DescriptionRules = None
    keywords: KeywordRules = None
    content: ContentRules = None

    # LLMé…ç½®
    llm_api_key: str = ""
    llm_base_url: str = "https://newapi.deepwisdom.ai/v1"
    llm_model: str = "gpt-4o"
    llm_timeout: int = 30
    llm_max_retries: int = 3
    enable_ai_analysis: bool = True

    def __post_init__(self):
        """åˆå§‹åŒ–å­è§„åˆ™"""
        if self.title is None:
            self.title = TitleRules()
        if self.description is None:
            self.description = DescriptionRules()
        if self.keywords is None:
            self.keywords = KeywordRules()
        if self.content is None:
            self.content = ContentRules()

    @classmethod
    def load_from_file(cls, config_file: str) -> 'MarkdownSEOConfig':
        """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config_data = json.load(f)

            # å®ä¾‹åŒ–å­è§„åˆ™
            title_rules = TitleRules(**config_data.get('title', {}))
            desc_rules = DescriptionRules(**config_data.get('description', {}))
            keyword_rules = KeywordRules(**config_data.get('keywords', {}))
            content_rules = ContentRules(**config_data.get('content', {}))

            # åˆ›å»ºä¸»é…ç½®
            config = cls(
                title=title_rules,
                description=desc_rules,
                keywords=keyword_rules,
                content=content_rules,
                llm_api_key=config_data.get('llm_api_key', ''),
                llm_base_url=config_data.get('llm_base_url', 'https://newapi.deepwisdom.ai/v1'),
                llm_model=config_data.get('llm_model', 'gpt-4o'),
                llm_timeout=config_data.get('llm_timeout', 30),
                llm_max_retries=config_data.get('llm_max_retries', 3),
                enable_ai_analysis=config_data.get('enable_ai_analysis', True)
            )

            logger.info(f"âœ… é…ç½®å·²ä»æ–‡ä»¶åŠ è½½: {config_file}")
            return config

        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤é…ç½®")
            return cls()

    @classmethod
    def load_from_env(cls) -> 'MarkdownSEOConfig':
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        config_path = os.getenv('SEO_RULES_CONFIG')
        if config_path and os.path.exists(config_path):
            return cls.load_from_file(config_path)

        # å°è¯•é»˜è®¤é…ç½®è·¯å¾„
        default_config = Path(__file__).parent.parent / 'config' / 'seo_rules.json'
        if default_config.exists():
            return cls.load_from_file(str(default_config))

        # ä½¿ç”¨é»˜è®¤é…ç½®
        config = cls()

        # ä»ç¯å¢ƒå˜é‡è¯»å–LLMé…ç½®
        config.llm_api_key = os.getenv('OPENAI_API_KEY', 'sk-tVlvoM4GZwWVT7GQWWcU8aD7J0pGguWBGiPFd6l4uF4JVMRM')
        config.llm_base_url = os.getenv('OPENAI_BASE_URL', 'https://newapi.deepwisdom.ai/v1')
        config.llm_model = os.getenv('OPENAI_MODEL', 'gpt-4o')

        logger.info("âœ… ä½¿ç”¨é»˜è®¤é…ç½®")
        return config

    def save_to_file(self, config_file: str):
        """ä¿å­˜é…ç½®åˆ°JSONæ–‡ä»¶"""
        config_data = {
            'title': asdict(self.title),
            'description': asdict(self.description),
            'keywords': asdict(self.keywords),
            'content': asdict(self.content),
            'llm_api_key': self.llm_api_key,
            'llm_base_url': self.llm_base_url,
            'llm_model': self.llm_model,
            'llm_timeout': self.llm_timeout,
            'llm_max_retries': self.llm_max_retries,
            'enable_ai_analysis': self.enable_ai_analysis
        }

        Path(config_file).parent.mkdir(parents=True, exist_ok=True)

        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {config_file}")
```

### 4.2 Markdownè§£æå™¨ (parsers.py)

```python
import frontmatter
import markdown
from bs4 import BeautifulSoup
from typing import Dict, Tuple, List
from collections import Counter
import re

# å‚è€ƒ: /mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/analyzer.py:16-94
def is_quality_keyword(keyword_phrase: str) -> bool:
    """
    è¿‡æ»¤é«˜è´¨é‡å…³é”®è¯

    æ‹’ç»:
    - URLç‰‡æ®µ (http://, www., .comç­‰)
    - HTML/CSSä»£ç  (<, >, {, }, class=ç­‰)
    - çº¯æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
    - åœç”¨è¯
    """
    words = keyword_phrase.lower().split()
    keyword_lower = keyword_phrase.lower()

    # Rule 0: æ‹’ç»URLç‰‡æ®µ
    url_indicators = ['http', 'https', 'www', '.com', '.net', '://']
    if any(indicator in keyword_lower for indicator in url_indicators):
        return False

    # Rule 1: æ‹’ç»HTML/CSSä»£ç ç‰‡æ®µ
    code_indicators = ['<', '>', '{', '}', 'class=', 'style=']
    if any(indicator in keyword_lower for indicator in code_indicators):
        return False

    # Rule 2: æ‹’ç»çº¯æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
    if keyword_phrase.replace(' ', '').replace('-', '').isdigit():
        return False

    # Rule 3: æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹
    special_char_count = sum(1 for c in keyword_phrase if not c.isalnum() and c != ' ')
    if special_char_count > len(keyword_phrase) * 0.3:
        return False

    # å•è¯æ£€æŸ¥
    if len(words) == 1:
        word = words[0]
        STOP_WORDS = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to'}
        if word in STOP_WORDS or len(word) < 3 or len(word) > 30:
            return False
        return True

    # å¤šè¯çŸ­è¯­æ£€æŸ¥
    if len(words) > 5:  # å¤ªé•¿çš„çŸ­è¯­ä¸æ˜¯å¥½å…³é”®è¯
        return False

    return True

class MarkdownParser:
    """Markdownè§£æå™¨"""

    def __init__(self):
        self.md = markdown.Markdown(extensions=['extra', 'codehilite', 'tables'])

    def parse_file(self, file_path: str) -> Tuple[Dict, str, str]:
        """
        è§£æMarkdownæ–‡ä»¶

        Returns:
            (frontmatter_dict, markdown_content, html_content)
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            post = frontmatter.load(f)

        frontmatter_dict = post.metadata
        markdown_content = post.content
        html_content = self.md.convert(markdown_content)

        return frontmatter_dict, markdown_content, html_content

    def extract_structure(self, html_content: str) -> Dict:
        """
        ä»HTMLæå–ç»“æ„ä¿¡æ¯

        Returns:
            {
                'h1_tags': [...],
                'h2_tags': [...],
                'images': [{'src': ..., 'alt': ...}],
                'links': [{'href': ..., 'text': ...}]
            }
        """
        soup = BeautifulSoup(html_content, 'html.parser')

        structure = {
            'h1_tags': [h1.get_text() for h1 in soup.find_all('h1')],
            'h2_tags': [h2.get_text() for h2 in soup.find_all('h2')],
            'h3_tags': [h3.get_text() for h3 in soup.find_all('h3')],
            'images': [
                {
                    'src': img.get('src', ''),
                    'alt': img.get('alt', ''),
                    'has_alt': bool(img.get('alt'))
                }
                for img in soup.find_all('img')
            ],
            'links': [
                {
                    'href': a.get('href', ''),
                    'text': a.get_text().strip()
                }
                for a in soup.find_all('a', href=True)
            ]
        }

        return structure

    def extract_keywords(self, text: str, top_n: int = 10) -> List[Dict]:
        """
        è‡ªåŠ¨æå–Topå…³é”®è¯

        Returns:
            [{'keyword': 'SEOå·¥å…·', 'count': 8, 'density': 0.8}, ...]
        """
        # åˆ†è¯ï¼ˆç®€å•å®ç°ï¼Œä»…è‹±æ–‡ï¼‰
        words = re.findall(r'\b\w+\b', text.lower())
        total_words = len(words)

        # ç»Ÿè®¡è¯é¢‘
        word_counter = Counter(words)

        # æå–åŒè¯ç»„
        bigrams = [' '.join(words[i:i+2]) for i in range(len(words)-1)]
        bigram_counter = Counter(bigrams)

        # åˆå¹¶å¹¶è¿‡æ»¤
        all_keywords = []
        for keyword, count in word_counter.most_common(top_n * 3):
            if is_quality_keyword(keyword):
                density = (count / total_words) * 100 if total_words > 0 else 0
                all_keywords.append({
                    'keyword': keyword,
                    'count': count,
                    'density': round(density, 2)
                })

        for keyword, count in bigram_counter.most_common(top_n * 2):
            if is_quality_keyword(keyword):
                # åŒè¯ç»„å¯†åº¦ = (count * 2) / total_words
                density = (count * 2 / total_words) * 100 if total_words > 0 else 0
                all_keywords.append({
                    'keyword': keyword,
                    'count': count,
                    'density': round(density, 2)
                })

        # æ’åºå¹¶è¿”å›Top N
        all_keywords.sort(key=lambda x: x['count'], reverse=True)
        return all_keywords[:top_n]
```

### 4.3 è§„åˆ™å¼•æ“ (rules_engine.py)

```python
from typing import List, Dict
from md_seo_agent.models import RuleCheckResult, Issue
from md_seo_agent.config import MarkdownSEOConfig

class RulesEngine:
    """è§„åˆ™æ£€æŸ¥å¼•æ“"""

    def __init__(self, config: MarkdownSEOConfig):
        self.config = config

    def check_all(
        self,
        frontmatter: Dict,
        html_content: str,
        structure: Dict,
        markdown_content: str,
        keywords_info: Dict
    ) -> Dict:
        """
        æ‰§è¡Œæ‰€æœ‰è§„åˆ™æ£€æŸ¥

        Returns:
            {
                'metadata_results': [...],
                'structure_results': [...],
                'keyword_results': [...]
            }
        """
        results = {
            'metadata_results': self.check_metadata(frontmatter),
            'structure_results': self.check_structure(structure),
            'keyword_results': self.check_keywords(keywords_info, frontmatter, markdown_content)
        }

        return results

    def check_metadata(self, frontmatter: Dict) -> List[RuleCheckResult]:
        """æ£€æŸ¥å…ƒæ•°æ® (META_*)"""
        results = []

        # META_01: Titleæ£€æŸ¥
        title = frontmatter.get('title', '')
        title_length = len(title)

        if title_length == 0:
            results.append(RuleCheckResult(
                rule_id="META_01",
                passed=False,
                score=0,
                severity="critical",
                message="Titleç¼ºå¤±",
                details={
                    'current_length': 0,
                    'expected_range': f"{self.config.title.min_length}-{self.config.title.max_length}"
                }
            ))
        elif title_length < self.config.title.min_length:
            priority = self.config.title.get_priority_for_length(title_length)
            results.append(RuleCheckResult(
                rule_id="META_01",
                passed=False,
                score=50,
                severity=priority,
                message=f"Titleè¿‡çŸ­ ({title_length}å­—ç¬¦ï¼Œå»ºè®®30-60)",
                details={'current': title, 'length': title_length}
            ))
        elif title_length > self.config.title.max_length:
            results.append(RuleCheckResult(
                rule_id="META_01",
                passed=False,
                score=50,
                severity="high",
                message=f"Titleè¿‡é•¿ ({title_length}å­—ç¬¦ï¼Œå»ºè®®30-60)",
                details={'current': title, 'length': title_length}
            ))
        else:
            results.append(RuleCheckResult(
                rule_id="META_01",
                passed=True,
                score=100,
                severity="low",
                message="Titleé•¿åº¦åˆé€‚",
                details={'current': title, 'length': title_length}
            ))

        # META_02: Descriptionæ£€æŸ¥
        description = frontmatter.get('description', '')
        desc_length = len(description)

        if desc_length == 0:
            results.append(RuleCheckResult(
                rule_id="META_02",
                passed=False,
                score=0,
                severity="high",
                message="Descriptionç¼ºå¤±",
                details={'expected_range': '120-160'}
            ))
        elif desc_length < self.config.description.min_length:
            results.append(RuleCheckResult(
                rule_id="META_02",
                passed=False,
                score=30,
                severity="medium",
                message=f"Descriptionè¿‡çŸ­ ({desc_length}å­—ç¬¦ï¼Œå»ºè®®120-160)",
                details={'current': description, 'length': desc_length}
            ))
        elif desc_length > self.config.description.max_length:
            results.append(RuleCheckResult(
                rule_id="META_02",
                passed=False,
                score=70,
                severity="medium",
                message=f"Descriptionè¿‡é•¿ ({desc_length}å­—ç¬¦ï¼Œå»ºè®®120-160)",
                details={'current': description, 'length': desc_length}
            ))
        else:
            results.append(RuleCheckResult(
                rule_id="META_02",
                passed=True,
                score=100,
                severity="low",
                message="Descriptioné•¿åº¦åˆé€‚",
                details={'current': description, 'length': desc_length}
            ))

        return results

    def check_structure(self, structure: Dict) -> List[RuleCheckResult]:
        """æ£€æŸ¥ç»“æ„ (STRUC_*)"""
        results = []

        # STRUC_01: H1æ ‡ç­¾æ£€æŸ¥
        h1_count = len(structure.get('h1_tags', []))

        if h1_count == 0:
            results.append(RuleCheckResult(
                rule_id="STRUC_01",
                passed=False,
                score=0,
                severity="high",
                message="ç¼ºå°‘H1æ ‡ç­¾",
                details={'count': 0}
            ))
        elif h1_count > 1:
            results.append(RuleCheckResult(
                rule_id="STRUC_01",
                passed=False,
                score=50,
                severity="medium",
                message=f"æ£€æµ‹åˆ°{h1_count}ä¸ªH1æ ‡ç­¾ï¼Œå»ºè®®åªæœ‰1ä¸ª",
                details={'count': h1_count, 'h1_list': structure['h1_tags']}
            ))
        else:
            results.append(RuleCheckResult(
                rule_id="STRUC_01",
                passed=True,
                score=100,
                severity="low",
                message="H1æ ‡ç­¾å”¯ä¸€æ€§ç¬¦åˆè¦æ±‚",
                details={'h1': structure['h1_tags'][0]}
            ))

        # STRUC_02: å›¾ç‰‡Altå±æ€§æ£€æŸ¥
        images = structure.get('images', [])
        total_images = len(images)
        images_with_alt = sum(1 for img in images if img.get('has_alt'))

        if total_images == 0:
            coverage = 100  # æ²¡æœ‰å›¾ç‰‡è§†ä¸º100%è¦†ç›–
        else:
            coverage = (images_with_alt / total_images) * 100

        if coverage < 50:
            results.append(RuleCheckResult(
                rule_id="STRUC_02",
                passed=False,
                score=0,
                severity="high",
                message=f"å›¾ç‰‡Altå±æ€§è¦†ç›–ç‡è¿‡ä½ ({coverage:.0f}%)",
                details={'total': total_images, 'with_alt': images_with_alt}
            ))
        elif coverage < 80:
            results.append(RuleCheckResult(
                rule_id="STRUC_02",
                passed=False,
                score=50,
                severity="medium",
                message=f"å›¾ç‰‡Altå±æ€§è¦†ç›–ç‡åä½ ({coverage:.0f}%)",
                details={'total': total_images, 'with_alt': images_with_alt}
            ))
        else:
            results.append(RuleCheckResult(
                rule_id="STRUC_02",
                passed=True,
                score=100,
                severity="low",
                message=f"å›¾ç‰‡Altå±æ€§è¦†ç›–ç‡è‰¯å¥½ ({coverage:.0f}%)",
                details={'total': total_images, 'with_alt': images_with_alt}
            ))

        # STRUC_03: é“¾æ¥å­˜åœ¨æ€§æ£€æŸ¥
        links = structure.get('links', [])
        has_links = len(links) > 0

        if not has_links:
            results.append(RuleCheckResult(
                rule_id="STRUC_03",
                passed=False,
                score=0,
                severity="medium",
                message="æ–‡ç« ç¼ºå°‘é“¾æ¥ï¼ˆå†…éƒ¨æˆ–å¤–éƒ¨ï¼‰",
                details={'count': 0}
            ))
        else:
            results.append(RuleCheckResult(
                rule_id="STRUC_03",
                passed=True,
                score=100,
                severity="low",
                message=f"æ–‡ç« åŒ…å«{len(links)}ä¸ªé“¾æ¥",
                details={'count': len(links)}
            ))

        return results

    def check_keywords(
        self,
        keywords_info: Dict,
        frontmatter: Dict,
        markdown_content: str
    ) -> List[RuleCheckResult]:
        """æ£€æŸ¥å…³é”®è¯ (KEY_*)"""
        results = []

        target_keyword = keywords_info.get('target_keyword')
        if not target_keyword:
            # æ— å…³é”®è¯æ—¶è·³è¿‡å…³é”®è¯æ£€æŸ¥
            return results

        keyword_data = keywords_info.get('keyword_data', {})
        density = keyword_data.get('density', 0)

        # KEY_01: å…³é”®è¯å¯†åº¦æ£€æŸ¥
        priority = self.config.keywords.get_priority_for_density(density)

        if density < self.config.keywords.min_density:
            results.append(RuleCheckResult(
                rule_id="KEY_01",
                passed=False,
                score=30,
                severity=priority,
                message=f"å…³é”®è¯å¯†åº¦è¿‡ä½ ({density:.2f}%ï¼Œå»ºè®®1%-2.5%)",
                details={'keyword': target_keyword, 'density': density}
            ))
        elif density > self.config.keywords.max_density:
            results.append(RuleCheckResult(
                rule_id="KEY_02",
                passed=False,
                score=50,
                severity=priority,
                message=f"å…³é”®è¯å¯†åº¦è¿‡é«˜ï¼Œå¯èƒ½è¢«åˆ¤å®šä¸ºå…³é”®è¯å †ç Œ ({density:.2f}%)",
                details={'keyword': target_keyword, 'density': density}
            ))
        else:
            results.append(RuleCheckResult(
                rule_id="KEY_01",
                passed=True,
                score=100,
                severity="low",
                message=f"å…³é”®è¯å¯†åº¦åˆé€‚ ({density:.2f}%)",
                details={'keyword': target_keyword, 'density': density}
            ))

        # KEY_02: å…³é”®è¯ä½ç½®æ£€æŸ¥
        title = frontmatter.get('title', '').lower()
        description = frontmatter.get('description', '').lower()
        first_paragraph = markdown_content[:500].lower()  # é¦–500å­—ç¬¦

        in_title = target_keyword.lower() in title
        in_description = target_keyword.lower() in description
        in_first_para = target_keyword.lower() in first_paragraph

        position_score = 0
        if in_title:
            position_score += 33
        if in_description:
            position_score += 33
        if in_first_para:
            position_score += 34

        severity = "low" if position_score >= 67 else "medium"

        results.append(RuleCheckResult(
            rule_id="KEY_02",
            passed=(position_score >= 67),
            score=position_score,
            severity=severity,
            message=f"å…³é”®è¯ä½ç½®è¦†ç›– {position_score}%",
            details={
                'keyword': target_keyword,
                'in_title': in_title,
                'in_description': in_description,
                'in_first_paragraph': in_first_para
            }
        ))

        return results
```

---

## 5. AIå¼•æ“è®¾è®¡ (ai_engine.py)

```python
import os
import asyncio
import json
import logging
from typing import Optional
from openai import AsyncOpenAI
from md_seo_agent.models import AIAnalysisResult
from md_seo_agent.config import MarkdownSEOConfig

logger = logging.getLogger(__name__)

class AIEngine:
    """AIè¯­ä¹‰åˆ†æå¼•æ“"""

    def __init__(self, config: MarkdownSEOConfig):
        self.config = config
        self.client = AsyncOpenAI(
            api_key=config.llm_api_key,
            base_url=config.llm_base_url,
            timeout=config.llm_timeout
        )

    async def analyze_semantics(
        self,
        content: str,
        keyword: Optional[str] = None
    ) -> Optional[AIAnalysisResult]:
        """
        LLMè¯­ä¹‰åˆ†æ

        Args:
            content: å†…å®¹æ–‡æœ¬ï¼ˆé™åˆ¶é•¿åº¦2000å­—ç¬¦ï¼‰
            keyword: å¯é€‰çš„ç›®æ ‡å…³é”®è¯

        Returns:
            AIAnalysisResult æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
        """
        content_sample = content[:2000]  # é™åˆ¶é•¿åº¦

        prompt = self._build_prompt(content_sample, keyword)

        for attempt in range(self.config.llm_max_retries):
            try:
                logger.info(f"ğŸ¤– è°ƒç”¨LLMåˆ†æ (å°è¯• {attempt + 1}/{self.config.llm_max_retries})...")

                response = await self.client.chat.completions.create(
                    model=self.config.llm_model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„SEOåˆ†æå¸ˆ"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"}
                )

                result_json = json.loads(response.choices[0].message.content)

                ai_result = AIAnalysisResult(
                    content_depth_score=result_json.get('content_depth_score', 5.0),
                    readability_score=result_json.get('readability_score', 5.0),
                    keyword_relevance_score=result_json.get('keyword_relevance_score'),
                    recommendations=result_json.get('recommendations', []),
                    analysis_details=result_json.get('details', {})
                )

                logger.info(f"âœ… LLMåˆ†ææˆåŠŸ: å†…å®¹æ·±åº¦{ai_result.content_depth_score}/10, å¯è¯»æ€§{ai_result.readability_score}/10")
                return ai_result

            except Exception as e:
                logger.warning(f"âš ï¸ LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.config.llm_max_retries}): {str(e)}")
                if attempt < self.config.llm_max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"âŒ LLMåˆ†ææœ€ç»ˆå¤±è´¥ï¼Œå°†é™çº§åˆ°çº¯è§„åˆ™åˆ†æ")
                    return None

    def _build_prompt(self, content: str, keyword: Optional[str]) -> str:
        """æ„å»ºLLM Prompt"""
        keyword_section = f"\nç›®æ ‡å…³é”®è¯: {keyword}" if keyword else "\nç›®æ ‡å…³é”®è¯: æ— ï¼ˆè¯·åŸºäºå†…å®¹è‡ªåŠ¨åˆ¤æ–­ï¼‰"

        prompt = f"""
ä½ æ˜¯SEOä¸“å®¶ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹Markdownå†…å®¹çš„è´¨é‡ã€‚

å†…å®¹æ‘˜è¦ï¼ˆå‰2000å­—ç¬¦ï¼‰:
{content}
{keyword_section}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†ï¼ˆ0-10åˆ†ï¼‰ï¼š
1. **å†…å®¹æ·±åº¦ä¸ä»·å€¼** (content_depth_score):
   - æ˜¯å¦æä¾›ç‹¬ç‰¹è§è§£ï¼Ÿ
   - æ˜¯å¦æœ‰å…·ä½“æ¡ˆä¾‹æˆ–æ•°æ®æ”¯æŒï¼Ÿ
   - æ˜¯å¦æ·±å…¥åˆ†æé—®é¢˜è€Œéæµ…å°è¾„æ­¢ï¼Ÿ

2. **é˜…è¯»æµç•…åº¦** (readability_score):
   - æ®µè½æ˜¯å¦æ¸…æ™°ç®€æ´ï¼Ÿ
   - å¥å­é•¿åº¦æ˜¯å¦é€‚ä¸­ï¼Ÿ
   - é€»è¾‘æ˜¯å¦è¿è´¯ï¼Ÿ

3. **å…³é”®è¯ç›¸å…³æ€§** (keyword_relevance_score, ä»…åœ¨æä¾›å…³é”®è¯æ—¶è¯„åˆ†):
   - å†…å®¹æ˜¯å¦å›´ç»•å…³é”®è¯å±•å¼€ï¼Ÿ
   - å…³é”®è¯ä½¿ç”¨æ˜¯å¦è‡ªç„¶ï¼Ÿ

4. **ä¼˜åŒ–å»ºè®®** (recommendations):
   - æä¾›3-5æ¡å…·ä½“çš„ä¼˜åŒ–å»ºè®®
   - å»ºè®®åº”å¯æ‰§è¡Œï¼Œéæ³›æ³›è€Œè°ˆ

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
{{
    "content_depth_score": 8.5,
    "readability_score": 9.0,
    "keyword_relevance_score": 7.5,
    "recommendations": [
        "å»ºè®®1ï¼šå¢åŠ å…·ä½“æ¡ˆä¾‹",
        "å»ºè®®2ï¼šç¼©çŸ­æ®µè½é•¿åº¦"
    ],
    "details": {{
        "strengths": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
        "weaknesses": ["å¾…æ”¹è¿›1", "å¾…æ”¹è¿›2"]
    }}
}}
"""
        return prompt
```

---

## 6. æ ¸å¿ƒåˆ†æå™¨ (analyzer.py)

```python
import asyncio
import logging
from pathlib import Path
from typing import Optional, Dict, List
from md_seo_agent.config import MarkdownSEOConfig
from md_seo_agent.models import SEOReport, ScoreBreakdown, Issue, Suggestion, AIAnalysisResult
from md_seo_agent.parsers import MarkdownParser
from md_seo_agent.rules_engine import RulesEngine
from md_seo_agent.ai_engine import AIEngine

logger = logging.getLogger(__name__)

class MarkdownSEOAnalyzer:
    """æ ¸å¿ƒåˆ†æå™¨"""

    def __init__(self, config: MarkdownSEOConfig):
        self.config = config
        self.parser = MarkdownParser()
        self.rules_engine = RulesEngine(config)
        self.ai_engine = AIEngine(config) if config.enable_ai_analysis else None

    def analyze_file(
        self,
        md_file_path: str,
        keyword: Optional[str] = None
    ) -> SEOReport:
        """
        åˆ†æMarkdownæ–‡ä»¶

        Args:
            md_file_path: Markdownæ–‡ä»¶è·¯å¾„
            keyword: å¯é€‰çš„ç›®æ ‡å…³é”®è¯

        Returns:
            SEOReportå¯¹è±¡
        """
        logger.info(f"ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: {md_file_path}")

        # 1. è§£æMarkdown
        frontmatter, markdown_content, html_content = self.parser.parse_file(md_file_path)
        structure = self.parser.extract_structure(html_content)

        # 2. å…³é”®è¯å¤„ç†
        keywords_info = self._process_keywords(markdown_content, keyword)

        # 3. è§„åˆ™æ£€æŸ¥
        rule_results = self.rules_engine.check_all(
            frontmatter,
            html_content,
            structure,
            markdown_content,
            keywords_info
        )

        # 4. AIè¯­ä¹‰æ£€æŸ¥ï¼ˆå¼‚æ­¥ï¼‰
        ai_result = None
        if self.ai_engine:
            ai_result = asyncio.run(
                self.ai_engine.analyze_semantics(markdown_content, keywords_info.get('target_keyword'))
            )

        # 5. è®¡ç®—æ€»åˆ†
        total_score, score_breakdown = self._calculate_score(rule_results, ai_result)

        # 6. ç”Ÿæˆé—®é¢˜å’Œå»ºè®®
        issues = self._generate_issues(rule_results, frontmatter, structure, keywords_info)
        suggestions = self._generate_suggestions(rule_results, ai_result)

        # 7. æ„å»ºæŠ¥å‘Š
        report = SEOReport(
            file_path=md_file_path,
            total_score=total_score,
            score_breakdown=score_breakdown,
            critical_issues=[i for i in issues if i.severity == 'critical'],
            high_priority_issues=[i for i in issues if i.severity == 'high'],
            medium_priority_issues=[i for i in issues if i.severity == 'medium'],
            low_priority_issues=[i for i in issues if i.severity == 'low'],
            suggestions=suggestions,
            ai_analysis=ai_result,
            keywords_analyzed=keywords_info.get('keywords_analyzed', []),
            auto_extracted_keywords=keywords_info.get('auto_extracted', False)
        )

        logger.info(f"âœ… åˆ†æå®Œæˆï¼Œæ€»åˆ†: {total_score:.1f}/100")
        return report

    def _process_keywords(self, markdown_content: str, user_keyword: Optional[str]) -> Dict:
        """å¤„ç†å…³é”®è¯ï¼šç”¨æˆ·æä¾›æˆ–è‡ªåŠ¨æå–"""
        if user_keyword:
            # ç”¨æˆ·æä¾›å…³é”®è¯
            keyword_count = markdown_content.lower().count(user_keyword.lower())
            total_words = len(markdown_content.split())
            density = (keyword_count / total_words) * 100 if total_words > 0 else 0

            return {
                'target_keyword': user_keyword,
                'keyword_data': {
                    'count': keyword_count,
                    'density': density
                },
                'keywords_analyzed': [user_keyword],
                'auto_extracted': False
            }
        else:
            # è‡ªåŠ¨æå–Topå…³é”®è¯
            extracted_keywords = self.parser.extract_keywords(markdown_content, top_n=10)
            top_keyword = extracted_keywords[0] if extracted_keywords else None

            if top_keyword:
                return {
                    'target_keyword': top_keyword['keyword'],
                    'keyword_data': {
                        'count': top_keyword['count'],
                        'density': top_keyword['density']
                    },
                    'keywords_analyzed': [kw['keyword'] for kw in extracted_keywords],
                    'auto_extracted': True
                }
            else:
                return {
                    'target_keyword': None,
                    'keywords_analyzed': [],
                    'auto_extracted': True
                }

    def _calculate_score(self, rule_results: Dict, ai_result: Optional[AIAnalysisResult]) -> tuple:
        """è®¡ç®—æ€»åˆ†å’Œåˆ†é¡¹å¾—åˆ†"""
        # å…ƒæ•°æ®å¾—åˆ† (æƒé‡30%)
        metadata_scores = [r.score for r in rule_results['metadata_results']]
        metadata_score = sum(metadata_scores) / len(metadata_scores) if metadata_scores else 0
        metadata_weighted = (metadata_score / 100) * 30

        # ç»“æ„å¾—åˆ† (æƒé‡25%)
        structure_scores = [r.score for r in rule_results['structure_results']]
        structure_score = sum(structure_scores) / len(structure_scores) if structure_scores else 0
        structure_weighted = (structure_score / 100) * 25

        # å…³é”®è¯å¾—åˆ† (æƒé‡20%)
        keyword_scores = [r.score for r in rule_results['keyword_results']]
        keyword_score = sum(keyword_scores) / len(keyword_scores) if keyword_scores else 100
        keyword_weighted = (keyword_score / 100) * 20

        # AIè¯­ä¹‰å¾—åˆ† (æƒé‡25%)
        if ai_result:
            # å†…å®¹æ·±åº¦15% + å¯è¯»æ€§10%
            ai_score = (ai_result.content_depth_score / 10 * 15) + (ai_result.readability_score / 10 * 10)
        else:
            ai_score = 0

        total_score = metadata_weighted + structure_weighted + keyword_weighted + ai_score

        # åˆ†é¡¹å¾—åˆ†è¯¦æƒ…
        score_breakdown = [
            ScoreBreakdown(
                dimension="å…ƒæ•°æ®",
                score=metadata_score,
                weight=0.3,
                weighted_score=metadata_weighted,
                status=self._get_status(metadata_score)
            ),
            ScoreBreakdown(
                dimension="ç»“æ„",
                score=structure_score,
                weight=0.25,
                weighted_score=structure_weighted,
                status=self._get_status(structure_score)
            ),
            ScoreBreakdown(
                dimension="å…³é”®è¯",
                score=keyword_score,
                weight=0.2,
                weighted_score=keyword_weighted,
                status=self._get_status(keyword_score)
            ),
            ScoreBreakdown(
                dimension="AIè¯­ä¹‰",
                score=ai_score / 0.25 if ai_score else 0,  # è½¬æ¢å›0-100
                weight=0.25,
                weighted_score=ai_score,
                status=self._get_status(ai_score / 0.25 if ai_score else 0)
            )
        ]

        return total_score, score_breakdown

    def _get_status(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è¿”å›çŠ¶æ€"""
        if score < 40:
            return "critical"
        elif score < 60:
            return "high"
        elif score < 80:
            return "medium"
        else:
            return "low"

    def _generate_issues(
        self,
        rule_results: Dict,
        frontmatter: Dict,
        structure: Dict,
        keywords_info: Dict
    ) -> List[Issue]:
        """ä»è§„åˆ™æ£€æŸ¥ç»“æœç”Ÿæˆé—®é¢˜åˆ—è¡¨"""
        issues = []

        # å…ƒæ•°æ®é—®é¢˜
        for result in rule_results['metadata_results']:
            if not result.passed:
                issue = self._rule_result_to_issue(result, frontmatter)
                if issue:
                    issues.append(issue)

        # ç»“æ„é—®é¢˜
        for result in rule_results['structure_results']:
            if not result.passed:
                issue = self._rule_result_to_issue(result, structure)
                if issue:
                    issues.append(issue)

        # å…³é”®è¯é—®é¢˜
        for result in rule_results['keyword_results']:
            if not result.passed:
                issue = self._rule_result_to_issue(result, keywords_info)
                if issue:
                    issues.append(issue)

        return issues

    def _rule_result_to_issue(self, result, context) -> Optional[Issue]:
        """å°†RuleCheckResultè½¬æ¢ä¸ºIssue"""
        # è¿™é‡Œæ ¹æ®å…·ä½“çš„rule_idç”Ÿæˆè¯¦ç»†çš„Issue
        # åŒ…å«ä¿®å¤ç¤ºä¾‹å’Œä½ç½®ä¿¡æ¯
        # å…·ä½“å®ç°ç•¥ï¼ˆCodexéœ€è¦è¡¥å……ï¼‰
        pass

    def _generate_suggestions(
        self,
        rule_results: Dict,
        ai_result: Optional[AIAnalysisResult]
    ) -> List[Suggestion]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []

        # ä»è§„åˆ™ç»“æœç”Ÿæˆå»ºè®®ï¼ˆç•¥ï¼‰
        # ä»AIç»“æœç”Ÿæˆå»ºè®®
        if ai_result and ai_result.recommendations:
            for rec in ai_result.recommendations:
                suggestions.append(Suggestion(
                    priority="medium",
                    category="content",
                    recommendation=rec,
                    rationale="AIåˆ†æå»ºè®®"
                ))

        return suggestions
```

---

## 7. æŠ¥å‘Šç”Ÿæˆå™¨ (reporter.py)

```python
from md_seo_agent.models import SEOReport
from datetime import datetime

class ReportGenerator:
    """MarkdownæŠ¥å‘Šç”Ÿæˆå™¨"""

    def generate_markdown_report(self, report: SEOReport) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""

        md_lines = []

        # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
        md_lines.append(f"# ğŸ“ SEOè¯Šæ–­æŠ¥å‘Š\n")
        md_lines.append(f"**æ–‡ä»¶**: `{report.file_path}`")
        md_lines.append(f"**åˆ†ææ—¶é—´**: {report.analysis_time.strftime('%Y-%m-%d %H:%M:%S')}")
        md_lines.append(f"**æ€»åˆ†**: {report.total_score:.1f}/100 {report.emoji_badge}\n")

        # è¯„åˆ†è¯¦æƒ…è¡¨æ ¼
        md_lines.append("## ğŸ“Š è¯„åˆ†è¯¦æƒ…\n")
        md_lines.append("| ç»´åº¦ | å¾—åˆ† | æƒé‡ | çŠ¶æ€ |")
        md_lines.append("|------|------|------|------|")

        for breakdown in report.score_breakdown:
            md_lines.append(
                f"| {breakdown.dimension} | {breakdown.score:.0f}/{breakdown.weight*100:.0f} | "
                f"{breakdown.weight*100:.0f}% | {breakdown.emoji} {breakdown.status} |"
            )

        md_lines.append("\n---\n")

        # ä¸¥é‡é—®é¢˜
        if report.critical_issues:
            md_lines.append("## ğŸ”´ ä¸¥é‡é—®é¢˜ (Critical)\n")
            for issue in report.critical_issues:
                md_lines.append(f"### {issue.id}: {issue.message}\n")
                md_lines.append(f"- **ä¸¥é‡ç¨‹åº¦**: {issue.severity}")
                if issue.current_value:
                    md_lines.append(f"- **å½“å‰å€¼**: {issue.current_value}")
                if issue.expected_value:
                    md_lines.append(f"- **å»ºè®®å€¼**: {issue.expected_value}")
                if issue.fix_example:
                    md_lines.append(f"- **ä¿®å¤ç¤ºä¾‹**:\n```yaml\n{issue.fix_example}\n```")
                md_lines.append("")

        # é«˜ä¼˜å…ˆçº§é—®é¢˜
        if report.high_priority_issues:
            md_lines.append("## ğŸŸ  é«˜ä¼˜å…ˆçº§è­¦å‘Š (High)\n")
            for issue in report.high_priority_issues:
                md_lines.append(f"### {issue.id}: {issue.message}\n")
                # ç±»ä¼¼æ ¼å¼
                md_lines.append("")

        # AIä¼˜åŒ–å»ºè®®
        if report.ai_analysis:
            md_lines.append("## ğŸ’¡ AIä¼˜åŒ–å»ºè®®\n")
            ai = report.ai_analysis
            md_lines.append(f"### å†…å®¹æ·±åº¦åˆ†æ ({ai.content_depth_score:.1f}/10)\n")
            md_lines.append("#### AIå»ºè®®:\n")
            for rec in ai.recommendations:
                md_lines.append(f"- {rec}")
            md_lines.append("")

        # å¿«é€Ÿä¿®å¤æ¸…å•
        md_lines.append("## ğŸ“ å¿«é€Ÿä¿®å¤æ¸…å•\n")
        for issue in (report.critical_issues + report.high_priority_issues):
            md_lines.append(f"- [ ] {issue.message}")

        return "\n".join(md_lines)

    def save_report(self, report: SEOReport, output_path: str):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        markdown_content = self.generate_markdown_report(report)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
```

---

## 8. CLIå…¥å£ (main.py)

```python
import argparse
import logging
from pathlib import Path
from md_seo_agent.config import MarkdownSEOConfig
from md_seo_agent.analyzer import MarkdownSEOAnalyzer
from md_seo_agent.reporter import ReportGenerator

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def main():
    parser = argparse.ArgumentParser(description='Markdown SEOè¯Šæ–­Agent')
    parser.add_argument('file', type=str, help='Markdownæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--keyword', type=str, help='ç›®æ ‡å…³é”®è¯ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--output', type=str, help='è¾“å‡ºæŠ¥å‘Šè·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    if args.config:
        config = MarkdownSEOConfig.load_from_file(args.config)
    else:
        config = MarkdownSEOConfig.load_from_env()

    # åˆ†ææ–‡ä»¶
    analyzer = MarkdownSEOAnalyzer(config)
    report = analyzer.analyze_file(args.file, args.keyword)

    # ç”ŸæˆæŠ¥å‘Š
    reporter = ReportGenerator()
    markdown_report = reporter.generate_markdown_report(report)

    # è¾“å‡º
    if args.output:
        reporter.save_report(report, args.output)
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
    else:
        print(markdown_report)

if __name__ == "__main__":
    main()
```

---

## 9. é…ç½®æ–‡ä»¶ç¤ºä¾‹ (config/seo_rules.json)

```json
{
  "title": {
    "min_length": 30,
    "max_length": 60,
    "ideal_min": 50,
    "ideal_max": 60,
    "critical_threshold": 30,
    "warning_threshold": 60
  },
  "description": {
    "min_length": 120,
    "max_length": 160,
    "ideal_min": 150,
    "ideal_max": 160,
    "critical_threshold": 120
  },
  "keywords": {
    "min_density": 0.01,
    "max_density": 0.025,
    "ideal_density": 0.015
  },
  "content": {
    "min_word_count": 300,
    "recommended_min": 500,
    "ideal_word_count": 1000
  },
  "llm_api_key": "",
  "llm_base_url": "https://newapi.deepwisdom.ai/v1",
  "llm_model": "gpt-4o",
  "llm_timeout": 30,
  "llm_max_retries": 3,
  "enable_ai_analysis": true
}
```

---

## 10. å…³é”®å‚è€ƒä»£ç ä½ç½®

| åŠŸèƒ½ | å‚è€ƒæ–‡ä»¶ | è¡Œå· | è¯´æ˜ |
|------|---------|------|------|
| é…ç½®ç³»ç»Ÿ | `/mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/seo_rules_config.py` | å…¨æ–‡ | dataclassè®¾è®¡æ¨¡å¼ |
| å…³é”®è¯è¿‡æ»¤ | `/mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/analyzer.py` | 16-94 | `is_quality_keyword()` å®ç° |
| Titleæ£€æŸ¥ | `/mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/analyzer.py` | 653-672 | `analyze_title()` å®ç° |
| Descriptionæ£€æŸ¥ | `/mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/analyzer.py` | 673-696 | `analyze_description()` å®ç° |
| æ•°æ®æ¨¡å‹ | `/mnt/d/SEO_develop/SEO-AutoPilot/pyseoanalyzer/page.py` | 88-216 | `Page` ç±»è®¾è®¡ |

---

**æ–‡æ¡£ç»“æŸ**

**ä¸‹ä¸€æ­¥**: è¯·å‚è€ƒ `CODEX_PROMPT.md` æŸ¥çœ‹è¯¦ç»†çš„å¼€å‘æŒ‡ä»¤ï¼Œä»¥åŠ `TEST_PLAN.md` æŸ¥çœ‹æµ‹è¯•è®¡åˆ’ã€‚
